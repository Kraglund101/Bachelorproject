{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80133060",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plots'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11352\\2452359610.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mplots\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_val_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_val_4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_loops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0meinops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRearrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plots'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Sampler\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomResizedCrop, ColorJitter, ToTensor, Normalize\n",
    "import pandas as pd\n",
    "from progressbar import ProgressBar \n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plots import plot_val_3, plot_val_4, plot_loops\n",
    "from torchsummary import summary\n",
    "from einops.layers.torch import Rearrange\n",
    "import csv\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "#original seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) \n",
    "\n",
    "class ClipOutput(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.clamp(x,min=0,max=1)\n",
    "    \n",
    "def seed_function():\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf2b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock_increasing(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels//4, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels//4)\n",
    "        self.conv2 = nn.Conv2d(out_channels//4, out_channels//2, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels//2)\n",
    "\n",
    "        \n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        \n",
    "        shortcut = self.shortcut(residual)\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        identity = self.shortcut(identity)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "class ResidualConvUnit(nn.Module):\n",
    "    \"\"\"Residual convolution module.\"\"\"\n",
    "\n",
    "    def __init__(self, features):\n",
    "        \"\"\"Init.\n",
    "        Args:\n",
    "            features (int): number of features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            features, features, kernel_size=3, stride=1, padding=1, bias=True\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            features, features, kernel_size=3, stride=1, padding=1, bias=True\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        Args:\n",
    "            x (tensor): input\n",
    "        Returns:\n",
    "            tensor: output\n",
    "        \"\"\"\n",
    "        out = self.relu(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        return out + x\n",
    "class FeatureFusionBlock(nn.Module):\n",
    "    \"\"\"Feature fusion block.\"\"\"\n",
    "\n",
    "    def __init__(self, features):\n",
    "        \"\"\"Init.\n",
    "        Args:\n",
    "            features (int): number of features\n",
    "        \"\"\"\n",
    "        super(FeatureFusionBlock, self).__init__()\n",
    "\n",
    "        self.resConfUnit1 = ResidualConvUnit(features)\n",
    "        self.resConfUnit2 = ResidualConvUnit(features)\n",
    "\n",
    "    def forward(self, *xs):\n",
    "        \"\"\"Forward pass.\n",
    "        Returns:\n",
    "            tensor: output\n",
    "        \"\"\"  \n",
    "        output = xs[0]\n",
    "\n",
    "        if len(xs) == 2:\n",
    "            output += self.resConfUnit1(xs[1])\n",
    "\n",
    "        output = self.resConfUnit2(output)\n",
    "\n",
    "        output = nn.functional.interpolate(\n",
    "            output, scale_factor=2, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6ec8f7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Rearrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11352\\357034726.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mViT_fusion_4_heads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[0mcount_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11352\\357034726.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, patch_size, hidden_dim, num_heads, num_layers)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRearrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'b c h w -> b (h w) c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpositional_encoding1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_patches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Rearrange' is not defined"
     ]
    }
   ],
   "source": [
    "class ViT_fusion_4_heads(nn.Module):\n",
    "    def __init__(self, patch_size = 16, hidden_dim=512, num_heads=4, num_layers=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.fusionblock4 = FeatureFusionBlock(256)\n",
    "        self.fusionblock3 = FeatureFusionBlock(256)\n",
    "        self.fusionblock2 = FeatureFusionBlock(256)\n",
    "        self.fusionblock1 = FeatureFusionBlock(256)\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(256, 256//2, kernel_size = 3, stride=1, padding=1),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\"),\n",
    "            nn.Conv2d(256//2, 32, kernel_size = 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 1, kernel_size = 1, stride = 1, padding= 0),\n",
    "            nn.ReLU(True),\n",
    "            nn.Identity()\n",
    "    \n",
    "        )\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_patches = (256 // patch_size)**2\n",
    "        self.proj1 = nn.Conv2d(3, hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        \n",
    "        self.resample1 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, 256, kernel_size = 1),\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        \n",
    "        self.resample2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, 256, kernel_size = 1),\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "\n",
    "        self.resample3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, 256, kernel_size = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        \n",
    "        self.resample4 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, 256, kernel_size = 1),\n",
    "            nn.Conv2d(256, 256, kernel_size = 2, stride = 2, padding= 0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        \n",
    "       \n",
    "        # Transformer encoder layers\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim*4),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.flatten = Rearrange('b c h w -> b (h w) c')\n",
    "        self.positional_encoding1 = nn.Parameter(torch.zeros(1, self.num_patches, hidden_dim))\n",
    "       \n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        intermediate_layers = [4,8,12,16]\n",
    "        output_intermediate_layers = []\n",
    "        \n",
    "        #32x3x256x256 -> 32x512x64x64\n",
    "        #print(\"pre_proj1\",x.shape)\n",
    "        x = self.proj1(x)\n",
    "        #print(\"post_proj1\",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        #print(\"pre_positional\",x.shape)\n",
    "        x = x + self.positional_encoding1\n",
    "        #print(\"post_positional\",x.shape)\n",
    "        \n",
    "        #transformer1\n",
    "        x = x.transpose(0,1)\n",
    "        for i, layer in enumerate(self.transformer.layers,1):\n",
    "            x = layer(x)\n",
    "            if i in intermediate_layers:\n",
    "                output_intermediate_layers.append(x)\n",
    "        \n",
    "        x1 = output_intermediate_layers[0].transpose(0,1)\n",
    "        x2 = output_intermediate_layers[1].transpose(0,1)\n",
    "        x3 = output_intermediate_layers[2].transpose(0,1)\n",
    "        x4 = output_intermediate_layers[3].transpose(0,1)\n",
    "            \n",
    "        x1 = x1.view(-1,16,16,512)\n",
    "        x2 = x2.view(-1,16,16,512)\n",
    "        x3 = x3.view(-1,16,16,512)\n",
    "        x4 = x4.view(-1,16,16,512)\n",
    "        \n",
    "        x1 = self.resample1(x1.permute(0,3,1,2))\n",
    "        x2 = self.resample2(x2.permute(0,3,1,2))\n",
    "        x3 = self.resample3(x3.permute(0,3,1,2))\n",
    "        x4 = self.resample4(x4.permute(0,3,1,2))\n",
    "        \n",
    "        path_4 = self.fusionblock4(x4)\n",
    "        path_3 = self.fusionblock3(path_4, x3)\n",
    "        path_2 = self.fusionblock2(path_3, x2)\n",
    "        path_1 = self.fusionblock1(path_2, x1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.head(path_1)\n",
    "       \n",
    "        \n",
    "        return output.squeeze(1)\n",
    "m = ViT_fusion_4_heads()   \n",
    "count_parameters(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "552042d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_fusion_8_heads(nn.Module):\n",
    "    def __init__(self, patch_size = 16, hidden_dim=512, num_heads=8, num_layers=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.fusionblock4 = FeatureFusionBlock(256)\n",
    "        self.fusionblock3 = FeatureFusionBlock(256)\n",
    "        self.fusionblock2 = FeatureFusionBlock(256)\n",
    "        self.fusionblock1 = FeatureFusionBlock(256)\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(256, 256//2, kernel_size = 3, stride=1, padding=1),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\"),\n",
    "            nn.Conv2d(256//2, 32, kernel_size = 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 1, kernel_size = 1, stride = 1, padding= 0),\n",
    "            nn.ReLU(True),\n",
    "            nn.Identity()\n",
    "    \n",
    "        )\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_patches = (256 // patch_size)**2\n",
    "        self.proj1 = nn.Conv2d(3, hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        \n",
    "        self.resample1 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, 256, kernel_size = 1),\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        \n",
    "        self.resample2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, 256, kernel_size = 1),\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "\n",
    "        self.resample3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, 256, kernel_size = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        \n",
    "        self.resample4 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, 256, kernel_size = 1),\n",
    "            nn.Conv2d(256, 256, kernel_size = 2, stride = 2, padding= 0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        \n",
    "       \n",
    "        # Transformer encoder layers\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim*4),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.flatten = Rearrange('b c h w -> b (h w) c')\n",
    "        self.positional_encoding1 = nn.Parameter(torch.zeros(1, self.num_patches, hidden_dim))\n",
    "       \n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        intermediate_layers = [4,8,12,16]\n",
    "        output_intermediate_layers = []\n",
    "        \n",
    "        #32x3x256x256 -> 32x512x64x64\n",
    "        #print(\"pre_proj1\",x.shape)\n",
    "        x = self.proj1(x)\n",
    "        #print(\"post_proj1\",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        #print(\"pre_positional\",x.shape)\n",
    "        x = x + self.positional_encoding1\n",
    "        #print(\"post_positional\",x.shape)\n",
    "        \n",
    "        #transformer1\n",
    "        x = x.transpose(0,1)\n",
    "        for i, layer in enumerate(self.transformer.layers,1):\n",
    "            x = layer(x)\n",
    "            if i in intermediate_layers:\n",
    "                output_intermediate_layers.append(x)\n",
    "        \n",
    "        x1 = output_intermediate_layers[0].transpose(0,1)\n",
    "        x2 = output_intermediate_layers[1].transpose(0,1)\n",
    "        x3 = output_intermediate_layers[2].transpose(0,1)\n",
    "        x4 = output_intermediate_layers[3].transpose(0,1)\n",
    "            \n",
    "        x1 = x1.view(-1,16,16,512)\n",
    "        x2 = x2.view(-1,16,16,512)\n",
    "        x3 = x3.view(-1,16,16,512)\n",
    "        x4 = x4.view(-1,16,16,512)\n",
    "        \n",
    "        x1 = self.resample1(x1.permute(0,3,1,2))\n",
    "        x2 = self.resample2(x2.permute(0,3,1,2))\n",
    "        x3 = self.resample3(x3.permute(0,3,1,2))\n",
    "        x4 = self.resample4(x4.permute(0,3,1,2))\n",
    "        \n",
    "        path_4 = self.fusionblock4(x4)\n",
    "        path_3 = self.fusionblock3(path_4, x3)\n",
    "        path_2 = self.fusionblock2(path_3, x2)\n",
    "        path_1 = self.fusionblock1(path_2, x1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.head(path_1)\n",
    "       \n",
    "        \n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506cd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_fusion_16_heads(nn.Module):\n",
    "    def __init__(self, patch_size = 16, hidden_dim=512, num_heads=16, num_layers=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.fusionblock4 = FeatureFusionBlock(256)\n",
    "        self.fusionblock3 = FeatureFusionBlock(256)\n",
    "        self.fusionblock2 = FeatureFusionBlock(256)\n",
    "        self.fusionblock1 = FeatureFusionBlock(256)\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(256, 256//2, kernel_size = 3, stride=1, padding=1),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\"),\n",
    "            nn.Conv2d(256//2, 32, kernel_size = 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 1, kernel_size = 1, stride = 1, padding= 0),\n",
    "            nn.ReLU(True),\n",
    "            nn.Identity()\n",
    "    \n",
    "        )\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_patches = (256 // patch_size)**2\n",
    "        self.proj1 = nn.Conv2d(3, hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        \n",
    "        self.resample1 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, 256, kernel_size = 1),\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.resample2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, 256, kernel_size = 1),\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.resample3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, 256, kernel_size = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.resample4 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, 256, kernel_size = 1),\n",
    "            nn.Conv2d(256, 256, kernel_size = 2, stride = 2, padding= 0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "        )\n",
    "        \n",
    "       \n",
    "        # Transformer encoder layers\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim*4),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.flatten = Rearrange('b c h w -> b (h w) c')\n",
    "        self.positional_encoding1 = nn.Parameter(torch.zeros(1, self.num_patches, hidden_dim))\n",
    "       \n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        intermediate_layers = [4,8,12,16]\n",
    "        output_intermediate_layers = []\n",
    "        \n",
    "        #32x3x256x256 -> 32x512x64x64\n",
    "        print(self.positional_encoding1.shape)\n",
    "        print(\"pre_proj1\",x.shape)\n",
    "        x = self.proj1(x)\n",
    "        print(\"post_proj1\",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        print(\"pre_positional\",x.shape)\n",
    "        x = x + self.positional_encoding1\n",
    "        print(\"post_positional\",x.shape)\n",
    "        \n",
    "        print(\"STOPPP\\n\")\n",
    "        #transformer1\n",
    "        x = x.transpose(0,1)\n",
    "        for i, layer in enumerate(self.transformer.layers,1):\n",
    "            x = layer(x)\n",
    "            if i in intermediate_layers:\n",
    "                output_intermediate_layers.append(x)\n",
    "        \n",
    "        x1 = output_intermediate_layers[0].transpose(0,1)\n",
    "        x2 = output_intermediate_layers[1].transpose(0,1)\n",
    "        x3 = output_intermediate_layers[2].transpose(0,1)\n",
    "        x4 = output_intermediate_layers[3].transpose(0,1)\n",
    "            \n",
    "        x1 = x1.view(-1,16,16,512)\n",
    "        x2 = x2.view(-1,16,16,512)\n",
    "        x3 = x3.view(-1,16,16,512)\n",
    "        x4 = x4.view(-1,16,16,512)\n",
    "        \n",
    "        x1 = self.resample1(x1.permute(0,3,1,2))\n",
    "        x2 = self.resample2(x2.permute(0,3,1,2))\n",
    "        x3 = self.resample3(x3.permute(0,3,1,2))\n",
    "        x4 = self.resample4(x4.permute(0,3,1,2))\n",
    "        \n",
    "        path_4 = self.fusionblock4(x4)\n",
    "        path_3 = self.fusionblock3(path_4, x3)\n",
    "        path_2 = self.fusionblock2(path_3, x2)\n",
    "        path_1 = self.fusionblock1(path_2, x1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.head(path_1)\n",
    "       \n",
    "        \n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1346eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc_conv1 = self._conv_block(3,96)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv2 = self._conv_block(96,192)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv3 = self._conv_block(192,384)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv4 = self._conv_block(384,768)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self._conv_block(768,1536)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(1536, 768, kernel_size=2, stride=2)\n",
    "        self.dec_conv4 = self._conv_block(1536, 768)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(768, 384, kernel_size=2, stride=2)\n",
    "        self.dec_conv3 = self._conv_block(768, 384)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(384, 192, kernel_size=2, stride=2)\n",
    "        self.dec_conv2 = self._conv_block(384, 192)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2)\n",
    "        self.dec_conv1 = self._conv_block(192, 96)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(96, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "           \n",
    "            \n",
    "        \n",
    "        #nn.init.kaiming_normal_(self.output.weight, nonlinearity=\"relu\")\n",
    "        \n",
    "        \n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels,out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "            \n",
    "        )\n",
    "        #for layer in conv_block:\n",
    "            #if isinstance(layer, nn.Conv2d):\n",
    "                #nn.init.kaiming_normal_(layer.weight, nonlinearity=\"relu\")\n",
    "        return conv_block\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "       # print(x.shape)\n",
    "        enc1 = self.enc_conv1(x)\n",
    "        #print(enc1.shape)\n",
    "        pool1 = self.pool1(enc1)\n",
    "        #print(pool1.shape)\n",
    "\n",
    "        enc2 = self.enc_conv2(pool1)\n",
    "        pool2 = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.enc_conv3(pool2)\n",
    "        pool3 = self.pool3(enc3)\n",
    "\n",
    "        enc4 = self.enc_conv4(pool3)\n",
    "        pool4 = self.pool4(enc4)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(pool4)\n",
    "        #print(bottleneck.shape)\n",
    "\n",
    "        # Decoder\n",
    "        upconv4 = self.upconv4(bottleneck)\n",
    "        merge4 = torch.cat([enc4, upconv4], dim=1)\n",
    "       # print(merge4.shape)\n",
    "        dec4 = self.dec_conv4(merge4)\n",
    "\n",
    "        upconv3 = self.upconv3(dec4)\n",
    "        merge3 = torch.cat([enc3, upconv3], dim=1)\n",
    "        dec3 = self.dec_conv3(merge3)\n",
    "\n",
    "        upconv2 = self.upconv2(dec3)\n",
    "        merge2 = torch.cat([enc2, upconv2], dim=1)\n",
    "        dec2 = self.dec_conv2(merge2)\n",
    "\n",
    "        upconv1 = self.upconv1(dec2)\n",
    "        merge1 = torch.cat([enc1, upconv1], dim=1)\n",
    "        dec1 = self.dec_conv1(merge1)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.output(dec1)\n",
    "        \n",
    "        #print(output.shape)\n",
    "        return output.squeeze(1)\n",
    "    \n",
    "c = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1cf5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeededShuffleSampler(Sampler):\n",
    "    def __init__(self, data_source, seed):\n",
    "        self.data_source = data_source\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Use torch.Generator() to create a new generator\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(self.seed)\n",
    "        return iter(torch.randperm(len(self.data_source), generator=g).tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None, epoch=0, global_seed=42):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        self.epoch = epoch\n",
    "        self.global_seed=global_seed\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #x = torch.Tensor(self.features[idx])\n",
    "        #y = torch.Tensor(self.targets[idx])\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            rng_state = torch.get_rng_state()\n",
    "\n",
    "            \n",
    "            seed = self.global_seed + self.epoch\n",
    "            torch.manual_seed(seed)\n",
    "            x = self.transform(x)\n",
    "\n",
    "\n",
    "            torch.manual_seed(seed)\n",
    "            y = self.transform(y)\n",
    "\n",
    "            torch.set_rng_state(rng_state)\n",
    "\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39334e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_with_crop = Compose([\n",
    "    RandomHorizontalFlip(0.5),\n",
    "    RandomResizedCrop(size=(256,256), scale=(0.8,1.2), antialias=True)\n",
    "])\n",
    "transform_no_crop = Compose([\n",
    "    RandomHorizontalFlip(0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a1767ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(size,choice):\n",
    "    \n",
    "    \n",
    "    size = size.lower()\n",
    "    \n",
    "  \n",
    "        \n",
    "    X_train = torch.load(f\"C:/Users/frede/Desktop/Bachelorprojekt/{choice}/{size}/X_train.pt\")\n",
    "    X_val = torch.load(f\"C:/Users/frede/Desktop/Bachelorprojekt/{choice}/{size}/X_val.pt\")\n",
    "    X_test_VALID = torch.load(f\"C:/Users/frede/Desktop/Bachelorprojekt/{choice}/{size}/X_test_VALID.pt\")\n",
    "    X_test_MIDAIR = torch.load(f\"C:/Users/frede/Desktop/Bachelorprojekt/{choice}/{size}/X_test_MIDAIR.pt\")\n",
    "\n",
    "    y_train = torch.load(f\"C:/Users/frede/Desktop/Bachelorprojekt/{choice}/{size}/y_train.pt\")\n",
    "    y_val = torch.load(f\"C:/Users/frede/Desktop/Bachelorprojekt/{choice}/{size}/y_val.pt\")\n",
    "    y_test_VALID = torch.load(f\"C:/Users/frede/Desktop/Bachelorprojekt/{choice}/{size}/y_test_VALID.pt\")\n",
    "    y_test_MIDAIR = torch.load(f\"C:/Users/frede/Desktop/Bachelorprojekt/{choice}/{size}/y_test_MIDAIR.pt\")\n",
    "\n",
    "    \n",
    "        \n",
    "    return (X_train,y_train), (X_val, y_val), (X_test_VALID, y_test_VALID), (X_test_MIDAIR, y_test_MIDAIR)\n",
    "    \n",
    "choice1 = \"Normalized_data_80\"\n",
    "choice2 = \"Normalized_standardized_data_80\"\n",
    "choice3 = \"Standardized_data_80\"\n",
    "choice4 = \"Normalized_standardized_data_80_new\"\n",
    "data = get_data(\"xsmall\", choice1)\n",
    "#training_data = MyDataset(data[0][0].float(),data[0][1].float(),transform=transform) \n",
    "X_train, y_train = data[0][0].float(), data[0][1].float()\n",
    "X_val, y_val = data[1][0].float(), data[1][1].float()\n",
    "X_test_VALID, y_test_VALID = data[2][0].float(), data[2][1].float()\n",
    "X_test_MIDAIR, y_test_MIDAIR = data[3][0].float(), data[3][1].float()\n",
    "\n",
    "minimum_depth_train = torch.min(y_train)\n",
    "maximum_depth_train = torch.max(y_train)\n",
    "minimum_depth_val = torch.min(y_val)\n",
    "maximum_depth_val = torch.max(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c94e32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(test_bool,train_loss, test_loss, settings, model_type, run_time, file_name):\n",
    "    \n",
    "    run_time = [run_time]\n",
    "    if test_bool:\n",
    "        epochs = train_loss\n",
    "        valid_absrel_loss, valid_mae_loss, valid_delta1_loss, valid_delta2_loss, valid_delta3_loss, midair_abs_loss, midair_mae_loss, midair_delta1_loss, midair_delta2_loss, midair_delta3_loss = settings\n",
    "    else:\n",
    "        data_size,epochs,batch_size,lr,l2_reg,dropout_bool,hidden_layers,nheads,encoder_layers = settings\n",
    "\n",
    "    if len(test_loss) > 1:\n",
    "        test_loss1, test_loss2 = test_loss[0], test_loss[1]\n",
    "        data = [test_loss1, valid_absrel_loss, valid_mae_loss, valid_delta1_loss, valid_delta2_loss, valid_delta3_loss, test_loss2,midair_abs_loss, midair_mae_loss, midair_delta1_loss, midair_delta2_loss, midair_delta3_loss,run_time]\n",
    "    else:\n",
    "        test_loss = test_loss[0]\n",
    "        data = [train_loss, test_loss, data_size, epochs, run_time, batch_size, lr, l2_reg, dropout_bool, hidden_layers, nheads, encoder_layers]\n",
    "\n",
    "    #print(data)\n",
    "    \n",
    "    \n",
    "    if epochs == [5]:\n",
    "        path = \"5epochs\"\n",
    "    elif epochs == [25]:\n",
    "        path = \"25epochs\"\n",
    "    elif epochs == [50]:\n",
    "        path = \"50epochs\"\n",
    "    elif epochs == [100]:\n",
    "        path = \"100epochs\"\n",
    "    else:\n",
    "        path = \"300epochs\"\n",
    "    \n",
    "    if model_type.lower() == \"cnn\":\n",
    "        final_path = os.path.join(path, \"cnns/csv\", file_name)\n",
    "    else:\n",
    "        final_path = os.path.join(path, \"vits/csv\", file_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open(final_path, \"w\", newline='') as file_name:\n",
    "        writer = csv.writer(file_name)\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c7fb60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(model, model_type, epochs, file_name,datasize):\n",
    "    \n",
    "    print(epochs)\n",
    "    if epochs == 5:\n",
    "        path = \"5epochs\"\n",
    "    elif epochs == 25:\n",
    "        path = \"25epochs\"\n",
    "    elif epochs == 50:\n",
    "        path = \"50epochs\"\n",
    "    elif epochs == 100:\n",
    "        path = \"100epochs\"\n",
    "    else:\n",
    "        path = \"300epochs\"\n",
    "    \n",
    "    if model_type.lower() == \"cnn\":\n",
    "        new_path = os.path.join(path, \"cnns/weights\",datasize)\n",
    "        if not os.path.exists(new_path):\n",
    "            os.makedirs(new_path)\n",
    "            final_path = os.path.join(new_path, file_name)\n",
    "            torch.save(model.state_dict(), final_path)\n",
    "        else:\n",
    "            final_path = os.path.join(new_path, file_name)\n",
    "            torch.save(model.state_dict(), final_path)\n",
    "            \n",
    "    else:\n",
    "        new_path = os.path.join(path, \"vits/weights\",datasize)\n",
    "        #print(new_path)\n",
    "        if not os.path.exists(new_path):\n",
    "            os.makedirs(new_path)\n",
    "            final_path = os.path.join(new_path, file_name)\n",
    "            torch.save(model.state_dict(), final_path)\n",
    "        else:\n",
    "            final_path = os.path.join(new_path, file_name)\n",
    "            torch.save(model.state_dict(), final_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "def norm(meter,min_val=0.1133,max_val=50.0):\n",
    "    return((meter-min_val)/(max_val-min_val))\n",
    "\n",
    "def reverse_norm(norm_val,min_val=0.1133,max_val=80.0):\n",
    "    return(norm_val*(max_val-min_val)+min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "606db376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WeightedRMSELoss1x(nn.Module):\n",
    "    def __init__(self,min_dist=0.1133, max_dist=2.0, weight=1, min_depth=0.1133, max_depth=80):\n",
    "        super(WeightedRMSELoss1x,self).__init__()\n",
    "        self.min_dist = (min_dist-min_depth)/(max_depth-min_depth)\n",
    "        self.max_dist = (max_dist-min_depth)/(max_depth-min_depth)\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self,y_pred,y_true):\n",
    "        squared_error = (y_pred - y_true)**2\n",
    "        \n",
    "        #creating a mask for depth values in the interval [0.1133, 2.0]\n",
    "        critical_mask = (y_true >= self.min_dist) & (y_true <= self.max_dist)\n",
    "        \n",
    "        weighted_squared_error = torch.where(critical_mask, squared_error*self.weight, squared_error)\n",
    "        \n",
    "        rmse = torch.sqrt(torch.mean(weighted_squared_error))\n",
    "        return rmse\n",
    "\n",
    "class WeightedRMSELoss2x(nn.Module):\n",
    "    def __init__(self,min_dist=0.1133, max_dist=2.0, weight=2, min_depth=0.1133, max_depth=80):\n",
    "        super(WeightedRMSELoss2x,self).__init__()\n",
    "        self.min_dist = (min_dist-min_depth)/(max_depth-min_depth)\n",
    "        self.max_dist = (max_dist-min_depth)/(max_depth-min_depth)\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self,y_pred,y_true):\n",
    "        squared_error = (y_pred - y_true)**2\n",
    "        \n",
    "        #creating a mask for depth values in the interval [0.1133, 2.0]\n",
    "        critical_mask = (y_true >= self.min_dist) & (y_true <= self.max_dist)\n",
    "        \n",
    "        weighted_squared_error = torch.where(critical_mask, squared_error*self.weight, squared_error)\n",
    "        \n",
    "        rmse = torch.sqrt(torch.mean(weighted_squared_error))\n",
    "        return rmse\n",
    "    \n",
    "class WeightedRMSELoss5x(nn.Module):\n",
    "    def __init__(self,min_dist=0.1133, max_dist=2.0, weight=5, min_depth=0.1133, max_depth=80):\n",
    "        super(WeightedRMSELoss5x,self).__init__()\n",
    "        self.min_dist = (min_dist-min_depth)/(max_depth-min_depth)\n",
    "        self.max_dist = (max_dist-min_depth)/(max_depth-min_depth)\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self,y_pred,y_true):\n",
    "        squared_error = (y_pred - y_true)**2\n",
    "        \n",
    "        #creating a mask for depth values in the interval [0.1133, 2.0]\n",
    "        critical_mask = (y_true >= self.min_dist) & (y_true <= self.max_dist)\n",
    "        \n",
    "        weighted_squared_error = torch.where(critical_mask, squared_error*self.weight, squared_error)\n",
    "        \n",
    "        rmse = torch.sqrt(torch.mean(weighted_squared_error))\n",
    "        return rmse   \n",
    "class WeightedRMSELoss10x(nn.Module):\n",
    "    def __init__(self,min_dist=0.1133, max_dist=2.0, weight=10, min_depth=0.1133, max_depth=80):\n",
    "        super(WeightedRMSELoss10x,self).__init__()\n",
    "        self.min_dist = (min_dist-min_depth)/(max_depth-min_depth)\n",
    "        self.max_dist = (max_dist-min_depth)/(max_depth-min_depth)\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self,y_pred,y_true):\n",
    "        squared_error = (y_pred - y_true)**2\n",
    "        \n",
    "        #creating a mask for depth values in the interval [0.1133, 2.0]\n",
    "        critical_mask = (y_true >= self.min_dist) & (y_true <= self.max_dist)\n",
    "        \n",
    "        weighted_squared_error = torch.where(critical_mask, squared_error*self.weight, squared_error)\n",
    "        \n",
    "        rmse = torch.sqrt(torch.mean(weighted_squared_error))\n",
    "        return rmse      \n",
    "\n",
    "class ClippedRMSELoss(nn.Module):\n",
    "    def __init__(self, min_value, max_value, penalty_weight):\n",
    "        super(ClippedRMSELoss,self).__init__()\n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "        self.penalty_weight = penalty_weight\n",
    "        \n",
    "    def forward(self,predictions, targets):\n",
    "        \n",
    "        rmse_loss = torch.sqrt(torch.mean((predictions-targets)**2))\n",
    "        upper_penalty = torch.relu(predictions-self.max_value)\n",
    "        penalty = torch.mean(upper_penalty)\n",
    "        loss = rmse_loss + self.penalty_weight*penalty\n",
    "        return loss\n",
    "    \n",
    "def log_grads(model, writer, epoch):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and \"bias\" not in name:\n",
    "            writer.add_scalar(f\"Gradients/{name}\", param.grad.abs().mean(), epoch)\n",
    "\n",
    "def get_optimizer(model,l2):\n",
    "    # Set different learning rates for the transformer backbone and the decoder\n",
    "    parameters = [\n",
    "        {\"params\": model.transformer.parameters(), \"lr\": 1e-5},\n",
    "        {\"params\": model.proj1.parameters(), \"lr\": 1e-5},\n",
    "        {\"params\": model.head.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.fusionblock1.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.fusionblock2.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.fusionblock3.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.fusionblock4.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.resample1.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.resample2.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.resample3.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.resample4.parameters(), \"lr\": 1e-4},\n",
    "    ]   \n",
    "    \n",
    "    optimizer = torch.optim.Adam(parameters,weight_decay=l2)\n",
    "    return optimizer\n",
    "\n",
    "def get_optimizer(model,l2):\n",
    "    # Set different learning rates for the transformer backbone and the decoder\n",
    "    parameters = [\n",
    "        {\"params\": model.enc_conv1.parameters(), \"lr\": 1e-5},\n",
    "        {\"params\": model.enc_conv2.parameters(), \"lr\": 1e-5},\n",
    "        {\"params\": model.enc_conv3.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.enc_conv4.parameters(), \"lr\": 1e-5},\n",
    "        {\"params\": model.bottleneck.parameters(), \"lr\": 1e-5},\n",
    "        {\"params\": model.upconv4.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.dec_conv4.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.upconv3.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.dec_conv3.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.upconv2.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.dec_conv2.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.upconv1.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.dec_conv1.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.output.parameters(), \"lr\": 1e-4},\n",
    "    ]   \n",
    "    \n",
    "    optimizer = torch.optim.Adam(parameters,weight_decay=l2)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\n",
    "def train_val(load, optim_split, loss_f, epochs, x_train, y_train, X_val, y_val, data_size, batch_size, model, lr, model_type, file_name, l2=True, dropout_bool=False, hidden_layers= 0, nheads=0, encoder_layers=0,transform=None):\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(load, map_location=device))\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"Parameter: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(f\"Initial values:\\n{param.data}\")\n",
    "        print(\"------------------\")\n",
    "\n",
    "    loss_function = loss_f.to(device)\n",
    "    initial_lr = lr\n",
    "    step_size = 10  # Number of epochs before reducing the learning rate\n",
    "    gamma = 0.66\n",
    "    \n",
    "    if l2:\n",
    "        if optim_split:\n",
    "            optimizer = get_optimizer(model,1e-5)\n",
    "            print(\"get_optimizer_chosen_l2\")\n",
    "        else:\n",
    "            optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=lr/10)\n",
    "    else:\n",
    "        if optim_split:\n",
    "            optimizer = get_optimizer(model,0)\n",
    "            print(\"get_optimizer_chosen\")\n",
    "        else:\n",
    "            optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=0)\n",
    "            \n",
    "    # Initialize the StepLR scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "    avg_train_loss_per_epoch_list = []\n",
    "    avg_test_loss_per_epoch_list = []\n",
    "    \n",
    "    testing_dataset = MyDataset(X_val,y_val)\n",
    "    testing_dl = DataLoader(testing_dataset,batch_size=batch_size)\n",
    "    \n",
    "    #start timer\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1,epochs+1):\n",
    "        rng_state = torch.get_rng_state()\n",
    "        training_dataset = MyDataset(x_train,y_train,transform,epoch,42)\n",
    "        torch.manual_seed(epoch+100)\n",
    "        training_dl = DataLoader(training_dataset,batch_size=batch_size, num_workers= 0, shuffle=True)\n",
    "        torch.set_rng_state(rng_state)\n",
    "\n",
    "        print(f\"current iteration: {epoch} / {epochs}\")\n",
    "        running_train_loss = 0.0\n",
    "        running_test_loss = 0.0\n",
    "        \n",
    "        # Setting model to training mode\n",
    "        model.train()\n",
    "        # Training loop\n",
    "        for i , (images, depth_images) in enumerate(training_dl):\n",
    "            images, depth_images = images.to(device), depth_images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).unsqueeze(1)\n",
    "            loss = loss_function(outputs,depth_images)\n",
    "            batch_loss = loss.item()\n",
    "            \n",
    "            running_train_loss += batch_loss\n",
    "            loss.backward()\n",
    "\n",
    "                \n",
    "            optimizer.step()\n",
    "            \n",
    "    \n",
    "        \n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Epoch {epoch}, Learning rate: {current_lr}\")\n",
    "        \n",
    "        scheduler.step()    \n",
    "        \n",
    "        avg_train_loss = running_train_loss/len(training_dl)\n",
    "        print(\"avg train loss\",avg_train_loss)\n",
    "        avg_train_loss_per_epoch_list.append(avg_train_loss)\n",
    "    \n",
    "        \n",
    "        model.eval()\n",
    "        # Validation / testing loop\n",
    "        with torch.no_grad():\n",
    "            for j, (images, depth_images) in enumerate(testing_dl):\n",
    "                           \n",
    "                # Moving images and depth images to GPU\n",
    "                images,depth_images = images.to(device), depth_images.to(device)\n",
    "                \n",
    "                \n",
    "                \n",
    "                #forward pass\n",
    "                outputs = model(images).unsqueeze(1)\n",
    "                #print(torch.max(outputs))\n",
    "                #print(torch.min(outputs))\n",
    "                \n",
    "                #loss = loss_function(outputs,depth_images)\n",
    "                loss = loss_function(outputs,depth_images)\n",
    "                #\n",
    "                batch_loss = loss.item()\n",
    "                \n",
    "                running_test_loss += batch_loss\n",
    "                \n",
    "        avg_test_loss = running_test_loss/len(testing_dl)\n",
    "        print(\"avg test loss\", avg_test_loss)\n",
    "        avg_test_loss_per_epoch_list.append(avg_test_loss)\n",
    "        #print(avg_test_loss, f\"epoch:{epoch}\")\n",
    "    \n",
    "    #stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time-start_time\n",
    "    \n",
    "    save_weights(model, model_type, epochs, file_name+\".pt\", data_size)\n",
    "    make_csv(avg_train_loss_per_epoch_list,[avg_test_loss_per_epoch_list],\n",
    "             ([data_size],[epoch],[batch_size],[lr],[l2],[dropout_bool],[hidden_layers],[nheads],[encoder_layers]),model_type, elapsed_time, file_name+\".csv\")\n",
    "    \n",
    "    \n",
    "    return(avg_train_loss_per_epoch_list, avg_test_loss_per_epoch_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc7781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d02db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_test(load, optim_split, loss_f, epochs, x_train, y_train, X_test_VALID, y_test_VALID, X_test_MIDAIR, y_test_MIDAIR, data_size, batch_size, model, lr, model_type, file_name, l2=True, dropout_bool=False, hidden_layers= 0, nheads=0, encoder_layers=0,transform=None):\n",
    "\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(load, map_location=device))\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"Parameter: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(f\"Initial values:\\n{param.data}\")\n",
    "        print(\"------------------\")\n",
    "\n",
    "    loss_function = loss_f.to(device)\n",
    "    initial_lr = lr\n",
    "    step_size = 10  # Number of epochs before reducing the learning rate\n",
    "    gamma = 0.66\n",
    "    \n",
    "    if l2:\n",
    "        if optim_split:\n",
    "            optimizer = get_optimizer(model,1e-5)\n",
    "            print(\"get_optimizer_chosen_l2\")\n",
    "        else:\n",
    "            optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=lr/10)\n",
    "    else:\n",
    "        if optim_split:\n",
    "            optimizer = get_optimizer(model,0)\n",
    "            print(\"get_optimizer_chosen\")\n",
    "        else:\n",
    "            optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=0)\n",
    "            \n",
    "    # Initialize the StepLR scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "    avg_train_loss_per_epoch_list = []\n",
    "    \n",
    "    testing_dataset_VALID = MyDataset(X_test_VALID,y_test_VALID, None,0,42)\n",
    "    testing_dl_VALID = DataLoader(testing_dataset_VALID,batch_size=16)\n",
    "    \n",
    "    testing_dataset_MIDAIR = MyDataset(X_test_MIDAIR,y_test_MIDAIR,None,0,42)\n",
    "    testing_dl_MIDAIR = DataLoader(testing_dataset_MIDAIR,batch_size=16)\n",
    "    \n",
    "    VALID_rmse_list = []\n",
    "    VALID_delta1_list, VALID_delta2_list, VALID_delta3_list = [],[],[]\n",
    "    VALID_mae_list, VALID_absrel_list = [],[]\n",
    "    MIDAIR_rmse_list = []\n",
    "    MIDAIR_delta1_list, MIDAIR_delta2_list, MIDAIR_delta3_list = [],[],[]\n",
    "    MIDAIR_mae_list, MIDAIR_absrel_list = [],[]\n",
    "\n",
    "    \n",
    "    #start timer\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1,epochs+1):\n",
    "        rng_state = torch.get_rng_state()\n",
    "        training_dataset = MyDataset(x_train,y_train,transform,epoch,42)\n",
    "        torch.manual_seed(epoch+100)\n",
    "        training_dl = DataLoader(training_dataset,batch_size=batch_size, num_workers= 0, shuffle=True)\n",
    "        torch.set_rng_state(rng_state)\n",
    "\n",
    "        print(f\"current iteration: {epoch} / {epochs}\")\n",
    "        running_train_loss = 0.0\n",
    "        running_test_loss = 0.0\n",
    "        \n",
    "        # Setting model to training mode\n",
    "        model.train()\n",
    "        # Training loop\n",
    "        for i , (images, depth_images) in enumerate(training_dl):\n",
    "            images, depth_images = images.to(device), depth_images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).unsqueeze(1)\n",
    "            #print(outputs.shape)\n",
    "            loss = loss_function(outputs,depth_images)\n",
    "            batch_loss = loss.item()\n",
    "            \n",
    "            running_train_loss += batch_loss\n",
    "            loss.backward()\n",
    "\n",
    "                \n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Epoch {epoch}, Learning rate: {current_lr}\")\n",
    "        \n",
    "        scheduler.step()    \n",
    "        \n",
    "        avg_train_loss = running_train_loss/len(training_dl)\n",
    "        print(\"avg train loss\",avg_train_loss)\n",
    "        avg_train_loss_per_epoch_list.append(avg_train_loss)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        VALID_rmse = 0\n",
    "        VALID_delta1, VALID_delta2, VALID_delta3 = 0, 0, 0\n",
    "        VALID_mae, VALID_absrel = 0, 0\n",
    "        \n",
    "        \n",
    "        #Setting model to evaluation mode\n",
    "        model.eval()\n",
    "        # Validation / testing loop\n",
    "        with torch.no_grad():\n",
    "            for j, (images, depth_images) in enumerate(testing_dl_VALID):\n",
    "    \n",
    "                # Moving images and depth images to GPU\n",
    "                images,depth_images = images.to(device), depth_images.to(device)\n",
    "                \n",
    "                #forward pass\n",
    "                outputs = model(images).unsqueeze(1)\n",
    "    \n",
    "                # Calculating loss\n",
    "                loss = WeightedRMSELoss1x()(outputs,depth_images)\n",
    "                batch_loss = loss.item()\n",
    "\n",
    "                VALID_rmse += batch_loss\n",
    "\n",
    "                #Calculate MAE\n",
    "                VALID_mae += torch.abs(outputs - depth_images).mean()\n",
    "\n",
    "                #Calculate AbsRel\n",
    "                depth_images[depth_images==-0.0008] = 0.01\n",
    "                \n",
    "                    \n",
    "  \n",
    "                VALID_absrel += torch.mean(torch.abs(outputs - depth_images) / depth_images)\n",
    "                #print(VALID_absrel)\n",
    "\n",
    "                #Calculate Delta\n",
    "                ratio = torch.max(outputs / depth_images, depth_images / outputs)\n",
    "                VALID_delta1 += (ratio < 1.25).float().mean()\n",
    "                VALID_delta2 += (ratio < 1.25**2).float().mean()\n",
    "                VALID_delta3 += (ratio < 1.25**3).float().mean()\n",
    "\n",
    "        VALID_rmse_list.append(VALID_rmse/len(testing_dl_VALID))\n",
    "        VALID_mae_list.append(VALID_mae.item()/len(testing_dl_VALID))\n",
    "        VALID_absrel_list.append(VALID_absrel.item()/len(testing_dl_VALID))\n",
    "        VALID_delta1_list.append(VALID_delta1.item()/len(testing_dl_VALID))\n",
    "        VALID_delta2_list.append(VALID_delta2.item()/len(testing_dl_VALID))\n",
    "        VALID_delta3_list.append(VALID_delta3.item()/len(testing_dl_VALID))\n",
    "        \n",
    "    \n",
    "        MIDAIR_rmse = 0\n",
    "        MIDAIR_delta1, MIDAIR_delta2, MIDAIR_delta3 = 0, 0, 0\n",
    "        MIDAIR_mae, MIDAIR_absrel = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for j, (images, depth_images) in enumerate(testing_dl_MIDAIR):\n",
    "\n",
    "                # Moving images and depth images to GPU\n",
    "                images,depth_images = images.to(device), depth_images.to(device)\n",
    "\n",
    "\n",
    "\n",
    "                #forward pass\n",
    "                outputs = model(images).unsqueeze(1)\n",
    "                # Calculating loss\n",
    "                loss = WeightedRMSELoss1x()(outputs,depth_images)\n",
    "                batch_loss = loss.item()\n",
    "\n",
    "                MIDAIR_rmse += batch_loss\n",
    "\n",
    "\n",
    "                #Calculate MAE\n",
    "                MIDAIR_mae += torch.abs(outputs - depth_images).mean()\n",
    "\n",
    "                #Calculate AbsRel\n",
    "                MIDAIR_absrel += torch.mean(torch.abs(outputs - depth_images) / depth_images)\n",
    "\n",
    "        \n",
    "                #Calculate Delta\n",
    "                ratio = torch.max(outputs / depth_images, depth_images / outputs)\n",
    "                #print(ratio)\n",
    "                MIDAIR_delta1 += (ratio < 1.25).float().mean()\n",
    "                MIDAIR_delta2 += (ratio < 1.25**2).float().mean()\n",
    "                MIDAIR_delta3 += (ratio < 1.25**3).float().mean()\n",
    "        #print(VALID_absrel.item()/len(testing_dl_MIDAIR))\n",
    "        MIDAIR_rmse_list.append(MIDAIR_rmse/len(testing_dl_MIDAIR))\n",
    "        MIDAIR_mae_list.append(MIDAIR_mae.item()/len(testing_dl_MIDAIR))\n",
    "        MIDAIR_absrel_list.append(MIDAIR_absrel.item()/len(testing_dl_MIDAIR))\n",
    "        MIDAIR_delta1_list.append(MIDAIR_delta1.item()/len(testing_dl_MIDAIR))\n",
    "        MIDAIR_delta2_list.append(MIDAIR_delta2.item()/len(testing_dl_MIDAIR))\n",
    "        MIDAIR_delta3_list.append(MIDAIR_delta3.item()/len(testing_dl_MIDAIR))\n",
    "    #print(MIDAIR_rmse_list)\n",
    "       #stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time-start_time\n",
    "    \n",
    "    save_weights(model, model_type, epochs, file_name+\".pt\", data_size)\n",
    "    make_csv(True,[epochs],[VALID_rmse_list,MIDAIR_rmse_list],\n",
    "             (VALID_absrel_list,VALID_mae_list,VALID_delta1_list,VALID_delta2_list,VALID_delta3_list,MIDAIR_absrel_list,MIDAIR_mae_list,MIDAIR_delta1_list,MIDAIR_delta2_list,MIDAIR_delta3_list), model_type, elapsed_time, file_name+\".csv\")\n",
    "    \n",
    "    \n",
    "    #return(avg_train_loss_per_epoch_list, avg_test_loss_per_epoch_list)\n",
    "#def full_test( l2=True, dropout_bool=False, hidden_layers= 0, nheads=0, encoder_layers=0,transform=None):\n",
    "#run = full_test(\"vit_fusion_4_heads_start_weights.pt\",False,nn.MSELoss(),50, X_train, y_train, X_test_VALID, y_test_VALID, X_test_MIDAIR,y_test_MIDAIR, \"extreme\", 16, ViT_fusion_4_heads(), 0.00005, \"vit\", \"vit_mse\", False, False, 512,4,16,transform_no_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f0d59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16fd563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\")\n",
    "#vit = ViT_fusion_4_heads().to(device)\n",
    "#vit.load_state_dict(torch.load(r\"50epochs\\vits\\weights\\extreme\\vit_fusion_extreme_4_heads_TEST.pt\"))\n",
    "\n",
    "#vit2 = ViT_fusion_16_heads().to(device)\n",
    "#vit2.load_state_dict(torch.load(r\"50epochs\\vits\\weights\\extreme\\vit_fusion_scheduled_extreme_16_heads_TEST.pt\"))\n",
    "#cnn = CNN().to(device)\n",
    "\n",
    "#cnn.load_state_dict(torch.load(r\"50epochs\\cnns\\weights\\extreme\\cnn_extreme_TEST.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f040b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(\"small\", choice4)\n",
    "#training_data = MyDataset(data[0][0].float(),data[0][1].float(),transform=transform) \n",
    "X_train, y_train = data[0][0].float(), data[0][1].float()\n",
    "X_val, y_val = data[1][0].float(), data[1][1].float()\n",
    "X_test_VALID, y_test_VALID = data[2][0].float(), data[2][1].float()\n",
    "X_test_MIDAIR, y_test_MIDAIR = data[3][0].float(), data[3][1].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c5c88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distribution(X_test_VALID, y_test_VALID, X_test_MIDAIR, y_test_MIDAIR):\n",
    "    testing_dataset_VALID = MyDataset(X_test_VALID, y_test_VALID, None, 0, 42)\n",
    "    testing_dl_VALID = DataLoader(testing_dataset_VALID, batch_size=1)\n",
    "\n",
    "    testing_dataset_MIDAIR = MyDataset(X_test_MIDAIR, y_test_MIDAIR, None, 0, 42)\n",
    "    testing_dl_MIDAIR = DataLoader(testing_dataset_MIDAIR, batch_size=1)\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    valid = []\n",
    "    midair = []\n",
    "    d = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (image, depth) in enumerate(testing_dl_VALID):\n",
    "            image, depth = image.to(device), depth.to(device)\n",
    "            depth = depth.squeeze(0).squeeze(0)\n",
    "            depth = depth.flatten()\n",
    "            valid.append(depth)    \n",
    "            d.append(depth)\n",
    "\n",
    "        for idx, (image, depth) in enumerate(testing_dl_MIDAIR):\n",
    "            image, depth = image.to(device), depth.to(device)\n",
    "            depth = depth.squeeze(0).squeeze(0)\n",
    "            depth = depth.flatten()\n",
    "            midair.append(depth)   \n",
    "            d.append(depth)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    return valid, midair, d\n",
    "\n",
    "d = find_distribution(X_test_VALID, y_test_VALID, X_test_MIDAIR, y_test_MIDAIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "287dd39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data1 = torch.stack(d[0])\\ndata1 = data1.view(-1)\\nsubsample_indices1 = torch.randint(0, len(data1), (len(data1),))\\nsubsample1 = data1[subsample_indices1]\\n# Convert the subsample to a NumPy array for plotting\\nsubsample1 = subsample1.detach().cpu().numpy()\\n# Define the number of bins for your histogram\\nnum_bins = 50\\n# Create and display the histogram\\nplt.hist(reverse_norm(subsample1), bins=num_bins,color=\"blue\")\\nplt.title(\"Distribution of Depth values in VALID\")\\nplt.ylabel(\"Number of Pixels\")\\nplt.xlabel(\"Depth value\")\\nplt.savefig(\"valid_depth_distribution\")\\nplt.show()'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"data1 = torch.stack(d[0])\n",
    "data1 = data1.view(-1)\n",
    "subsample_indices1 = torch.randint(0, len(data1), (len(data1),))\n",
    "subsample1 = data1[subsample_indices1]\n",
    "# Convert the subsample to a NumPy array for plotting\n",
    "subsample1 = subsample1.detach().cpu().numpy()\n",
    "# Define the number of bins for your histogram\n",
    "num_bins = 50\n",
    "# Create and display the histogram\n",
    "plt.hist(reverse_norm(subsample1), bins=num_bins,color=\"blue\")\n",
    "plt.title(\"Distribution of Depth values in VALID\")\n",
    "plt.ylabel(\"Number of Pixels\")\n",
    "plt.xlabel(\"Depth value\")\n",
    "plt.savefig(\"valid_depth_distribution\")\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "134bcdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data2 = torch.stack(d[1])\\ndata2 = data2.view(-1)\\nsubsample_indices2 = torch.randint(0, len(data2), (len(data2),))\\nsubsample2 = data2[subsample_indices2]\\n# Convert the subsample to a NumPy array for plotting\\nsubsample2 = subsample2.detach().cpu().numpy()\\n# Define the number of bins for your histogram\\nnum_bins = 50\\n# Create and display the histogram\\nplt.hist(reverse_norm(subsample2), bins=num_bins,color=\"green\")\\n#plt.hist(reverse_norm(subsample1), bins=num_bins,color=\"blue\",alpha=0.5)\\nplt.title(\"Distribution of Depth values in Mid-Air\")\\nplt.ylabel(\"Number of Pixels\")\\nplt.xlabel(\"Depth value\")\\nplt.savefig(\"midair_depth_distribution\")\\nplt.show()'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"data2 = torch.stack(d[1])\n",
    "data2 = data2.view(-1)\n",
    "subsample_indices2 = torch.randint(0, len(data2), (len(data2),))\n",
    "subsample2 = data2[subsample_indices2]\n",
    "# Convert the subsample to a NumPy array for plotting\n",
    "subsample2 = subsample2.detach().cpu().numpy()\n",
    "# Define the number of bins for your histogram\n",
    "num_bins = 50\n",
    "# Create and display the histogram\n",
    "plt.hist(reverse_norm(subsample2), bins=num_bins,color=\"green\")\n",
    "#plt.hist(reverse_norm(subsample1), bins=num_bins,color=\"blue\",alpha=0.5)\n",
    "plt.title(\"Distribution of Depth values in Mid-Air\")\n",
    "plt.ylabel(\"Number of Pixels\")\n",
    "plt.xlabel(\"Depth value\")\n",
    "plt.savefig(\"midair_depth_distribution\")\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26b34a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data3 = torch.stack(d[2])\\ndata3 = data3.view(-1)\\nsubsample_indices3 = torch.randint(0, len(data3), (len(data3),))\\nsubsample3 = data3[subsample_indices3]\\n# Convert the subsample to a NumPy array for plotting\\nsubsample3 = subsample3.detach().cpu().numpy()\\n# Define the number of bins for your histogram\\nnum_bins = 50\\n# Create and display the histogram\\nplt.hist(reverse_norm(subsample3), bins=num_bins,color=\"orange\")\\nplt.title(\"Depth Values from Combined dataset\")\\nplt.ylabel(\"Number of Pixels\")\\nplt.xlabel(\"Depth value\")\\nplt.savefig(\"combined_depth_distribution\")\\nplt.show()'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"data3 = torch.stack(d[2])\n",
    "data3 = data3.view(-1)\n",
    "subsample_indices3 = torch.randint(0, len(data3), (len(data3),))\n",
    "subsample3 = data3[subsample_indices3]\n",
    "# Convert the subsample to a NumPy array for plotting\n",
    "subsample3 = subsample3.detach().cpu().numpy()\n",
    "# Define the number of bins for your histogram\n",
    "num_bins = 50\n",
    "# Create and display the histogram\n",
    "plt.hist(reverse_norm(subsample3), bins=num_bins,color=\"orange\")\n",
    "plt.title(\"Depth Values from Combined dataset\")\n",
    "plt.ylabel(\"Number of Pixels\")\n",
    "plt.xlabel(\"Depth value\")\n",
    "plt.savefig(\"combined_depth_distribution\")\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aeef3d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Set the figure size\\nplt.figure(figsize=(16, 12))\\n# Plot red and blue histograms together\\nplt.subplot(2, 2, 1)\\nplt.hist(reverse_norm(subsample1), bins=50, alpha=0.5, color=\\'b\\', label=\\'VALID\\')\\nplt.hist(reverse_norm(subsample2), bins=50, alpha=0.5, color=\\'g\\', label=\\'Mid-Air\\')\\nplt.title(\"Depth Values plotted together\")\\nplt.ylabel(\"Number of Pixels\")\\nplt.xlabel(\"Depth value\")\\nplt.legend()\\n\\nplt.subplot(2, 2, 2)\\nplt.hist(reverse_norm(subsample3), bins=num_bins,color=\"orange\")\\nplt.title(\"Depth Values from Combined dataset\")\\nplt.ylabel(\"Number of Pixels\")\\nplt.xlabel(\"Depth value\")\\nplt.subplots_adjust(bottom=0.1)\\nplt.tight_layout()\\nplt.savefig(\"all_distributions\", bbox_inches=\\'tight\\')\\n\\nplt.show()\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Set the figure size\n",
    "plt.figure(figsize=(16, 12))\n",
    "# Plot red and blue histograms together\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(reverse_norm(subsample1), bins=50, alpha=0.5, color='b', label='VALID')\n",
    "plt.hist(reverse_norm(subsample2), bins=50, alpha=0.5, color='g', label='Mid-Air')\n",
    "plt.title(\"Depth Values plotted together\")\n",
    "plt.ylabel(\"Number of Pixels\")\n",
    "plt.xlabel(\"Depth value\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(reverse_norm(subsample3), bins=num_bins,color=\"orange\")\n",
    "plt.title(\"Depth Values from Combined dataset\")\n",
    "plt.ylabel(\"Number of Pixels\")\n",
    "plt.xlabel(\"Depth value\")\n",
    "plt.subplots_adjust(bottom=0.1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"all_distributions\", bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fad4c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Now you can plot your losses\\nplt.figure(figsize=(20, 12))\\n#vit_valid_losses_new, cnn_valid_losses_new, vit_midair_losses_new, cnn_midair_losses_new\\nplt.subplot(2, 2, 1)\\n#plt.hist(vit_valid_losses, bins=50, alpha=0.5, color='r', label='ViT_VALID')\\nplt.hist([reverse_norm(loss) for loss in vit_valid_losses], bins=50, alpha=0.5, color='r', label='ViT_VALID')\\nplt.xlabel('Loss in meters')\\nplt.ylabel('Count')\\nplt.legend()\\n\\nplt.subplot(2, 2, 2)\\n#plt.hist(vit2_valid_losses, bins=50, alpha=0.5, color='b', label='ViT2_VALID')\\nplt.hist([reverse_norm(loss) for loss in vit2_valid_losses], bins=50, alpha=0.5, color='b', label='vit2_VALID')\\nplt.xlabel('Loss in meters')\\nplt.ylabel('Count')\\nplt.legend()\\n\\nplt.subplot(2, 2, 3)\\n#plt.hist(vit_midair_losses, bins=50, alpha=0.5, color='g', label='ViT_Mid-Air')\\nplt.hist([reverse_norm(loss) for loss in vit_midair_losses], bins=50, alpha=0.5, color='g', label='ViT_Mid-Air')\\nplt.xlabel('Loss in meters')\\nplt.ylabel('Count')\\nplt.legend()\\n\\nplt.subplot(2, 2, 4)\\n#plt.hist(vit2_midair_losses, bins=50, alpha=0.5, color='y', label='ViT2_Mid-Air')\\nplt.hist([reverse_norm(loss) for loss in vit2_midair_losses], bins=50, alpha=0.5, color='y', label='vit2_Mid-Air')\\nplt.xlabel('Loss in meters')\\nplt.ylabel('Count')\\nplt.legend()\\n\\nplt.tight_layout()\\n\\n\\n# Save the plot as a PNG file\\n#plt.savefig('loss_distribution.png', dpi=300, format='png')\\nplt.show()\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Now you can plot your losses\n",
    "plt.figure(figsize=(20, 12))\n",
    "#vit_valid_losses_new, cnn_valid_losses_new, vit_midair_losses_new, cnn_midair_losses_new\n",
    "plt.subplot(2, 2, 1)\n",
    "#plt.hist(vit_valid_losses, bins=50, alpha=0.5, color='r', label='ViT_VALID')\n",
    "plt.hist([reverse_norm(loss) for loss in vit_valid_losses], bins=50, alpha=0.5, color='r', label='ViT_VALID')\n",
    "plt.xlabel('Loss in meters')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "#plt.hist(vit2_valid_losses, bins=50, alpha=0.5, color='b', label='ViT2_VALID')\n",
    "plt.hist([reverse_norm(loss) for loss in vit2_valid_losses], bins=50, alpha=0.5, color='b', label='vit2_VALID')\n",
    "plt.xlabel('Loss in meters')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "#plt.hist(vit_midair_losses, bins=50, alpha=0.5, color='g', label='ViT_Mid-Air')\n",
    "plt.hist([reverse_norm(loss) for loss in vit_midair_losses], bins=50, alpha=0.5, color='g', label='ViT_Mid-Air')\n",
    "plt.xlabel('Loss in meters')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "#plt.hist(vit2_midair_losses, bins=50, alpha=0.5, color='y', label='ViT2_Mid-Air')\n",
    "plt.hist([reverse_norm(loss) for loss in vit2_midair_losses], bins=50, alpha=0.5, color='y', label='vit2_Mid-Air')\n",
    "plt.xlabel('Loss in meters')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "#plt.savefig('loss_distribution.png', dpi=300, format='png')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0399a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Set the figure size\\nplt.figure(figsize=(12, 10))\\n\\n# Plot red and blue histograms together\\nplt.subplot(2, 2, 1)\\n#plt.hist([reverse_norm(x) for x in vit_valid_no_extreme_outliers], bins=100, alpha=0.5, color='r', label='ViT_VALID')\\nplt.hist([reverse_norm(x) for x in vit_valid_losses], bins=100, alpha=0.5, color='r', label='ViT_4_heads VALID')\\nplt.hist([reverse_norm(x) for x in vit2_valid_losses], bins=100, alpha=0.5, color='b', label='ViT_16_heads VALID')\\nplt.xlabel('Loss in meters')\\nplt.ylabel('Count')\\nplt.legend()\\n\\n# Plot red and yellow histograms together\\nplt.subplot(2, 2, 2)\\nplt.hist([reverse_norm(x) for x in vit_midair_losses], bins=100, alpha=0.5, color='orange', label='ViT_4_heads Mid-Air')\\nplt.hist([reverse_norm(x) for x in vit2_midair_losses], bins=100, alpha=0.5, color='g', label='ViT_16_heads Mid-Air')\\nplt.xlabel('Loss in meters')\\nplt.ylabel('Count')\\nplt.legend()\\n\\n# Add a title to your plot\\n#plt.suptitle('Loss Distributions with 9 extreme outliers removed from ViT_VALID')\\n\\n#plt.tight_layout()\\n\\n# Save the plot as a PNG file\\nplt.savefig('loss_distribution_vit_valid_removed_outliers_4_16.png', dpi=300, format='png', bbox_inches='tight', pad_inches=0)\\n\\nplt.show()\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Set the figure size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot red and blue histograms together\n",
    "plt.subplot(2, 2, 1)\n",
    "#plt.hist([reverse_norm(x) for x in vit_valid_no_extreme_outliers], bins=100, alpha=0.5, color='r', label='ViT_VALID')\n",
    "plt.hist([reverse_norm(x) for x in vit_valid_losses], bins=100, alpha=0.5, color='r', label='ViT_4_heads VALID')\n",
    "plt.hist([reverse_norm(x) for x in vit2_valid_losses], bins=100, alpha=0.5, color='b', label='ViT_16_heads VALID')\n",
    "plt.xlabel('Loss in meters')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# Plot red and yellow histograms together\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist([reverse_norm(x) for x in vit_midair_losses], bins=100, alpha=0.5, color='orange', label='ViT_4_heads Mid-Air')\n",
    "plt.hist([reverse_norm(x) for x in vit2_midair_losses], bins=100, alpha=0.5, color='g', label='ViT_16_heads Mid-Air')\n",
    "plt.xlabel('Loss in meters')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# Add a title to your plot\n",
    "#plt.suptitle('Loss Distributions with 9 extreme outliers removed from ViT_VALID')\n",
    "\n",
    "#plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('loss_distribution_vit_valid_removed_outliers_4_16.png', dpi=300, format='png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1806150b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11352\\4250847963.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;31m#for i in range(500):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m \u001b[0mshow_index_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvit2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"midair\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_VALID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_VALID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_MIDAIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_MIDAIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'vit' is not defined"
     ]
    }
   ],
   "source": [
    "def show_index_2(vit_model, cnn_model, choose, index, X_test_VALID, y_test_VALID, X_test_MIDAIR, y_test_midair):\n",
    "    nrows = 1\n",
    "    ncolumns = 4\n",
    "    subplot_width = 2.7\n",
    "    subplot_height = 2.7 \n",
    "    extra_width_space = 0.5\n",
    "    extra_height_space = 0.5\n",
    "    fig_width = ncolumns*subplot_width + extra_width_space\n",
    "    fig_height = nrows*subplot_height + extra_height_space\n",
    "    fig_size = (fig_width, fig_height)\n",
    "    \n",
    "    if choose.lower() == \"valid\":\n",
    "        testing_dataset = MyDataset(X_test_VALID,y_test_VALID, None,0, 42)\n",
    "        testing_dl = DataLoader(testing_dataset,batch_size=1)\n",
    "    else:\n",
    "        testing_dataset = MyDataset(X_test_MIDAIR,y_test_MIDAIR,None,0,42)\n",
    "        testing_dl = DataLoader(testing_dataset,batch_size=1)\n",
    "    \n",
    "    fig1, axes1 = plt.subplots(nrows,ncolumns, figsize=fig_size)\n",
    "    fig1.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "    vit_model.eval()\n",
    "    cnn_model.eval()\n",
    "    loss_function = WeightedRMSELoss1x().to(device)\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, depth) in enumerate(Subset(testing_dataset,[index])):   \n",
    "            image, depth = image.to(device), depth.to(device)\n",
    "            image = image.unsqueeze(0)\n",
    "            vit_depth_pred = vit_model(image).squeeze(0) \n",
    "            cnn_depth_pred = cnn_model(image).squeeze(0)\n",
    "            print(\"vit loss\", loss_function(vit_depth_pred,depth))\n",
    "            print(\"cnn loss\", loss_function(cnn_depth_pred,depth))\n",
    "\n",
    "            image[:,0,:,:] = image[:,0,:,:]*72.5+79.88429260253906\n",
    "            image[:,1,:,:] = image[:,1,:,:]*67.4375+112.80674743652344\n",
    "            image[:,2,:,:] = image[:,2,:,:]*72.4375+109.85155487060547\n",
    "\n",
    "            image_normalized = (image.cpu().squeeze(0))/287\n",
    "            cbar_ax = fig1.add_axes([0.12, 0.1, 0.785, 0.05]) \n",
    "            \n",
    "            # for original image\n",
    "            axes1[0].imshow(image_normalized.squeeze(0).permute(1,2,0))\n",
    "            axes1[0].set_title(\"Original_Image\")\n",
    "            axes1[0].axis(\"off\")\n",
    "\n",
    "            # for ViT prediction\n",
    "            img2 = axes1[1].imshow(reverse_norm(vit_depth_pred.cpu().numpy()), vmin=0, vmax=reverse_norm(1))\n",
    "            axes1[1].set_title(\"ViT_Prediction (4 heads)\")\n",
    "            axes1[1].axis(\"off\")\n",
    "\n",
    "            # for CNN prediction\n",
    "            img3 = axes1[2].imshow(reverse_norm(cnn_depth_pred.cpu().numpy()), vmin=0, vmax=reverse_norm(1))\n",
    "            axes1[2].set_title(\"ViT_Prediction (16 heads)\")\n",
    "            axes1[2].axis(\"off\")\n",
    "\n",
    "            # for ground truth\n",
    "            img4 = axes1[3].imshow(reverse_norm(depth.squeeze(0).squeeze(0).cpu().numpy()), vmin=0, vmax=reverse_norm(1))\n",
    "            axes1[3].set_title(\"Ground_Truth\")\n",
    "            axes1[3].axis(\"off\")\n",
    "\n",
    "            # Single colorbar for all subplots\n",
    "            cbar = fig1.colorbar(img4, cax=cbar_ax, orientation='horizontal')\n",
    "            cbar.ax.set_xticklabels(['{:.1f} m'.format(i) for i in cbar.get_ticks()])\n",
    "\n",
    "    fig1.show()\n",
    "    \n",
    "    #fig1.savefig(\"midair_7_vit_superior.png\", bbox_inches=\"tight\")  # Save the figure with tight bounding box\n",
    "#for i in range(500):\n",
    "    \n",
    "show_index_2(vit,vit2,\"midair\",1,X_test_VALID, y_test_VALID, X_test_MIDAIR, y_test_MIDAIR)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e5a3316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Rearrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11352\\438691789.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0mfig2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m \u001b[0mvit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mViT_fusion_4_heads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[0mvit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"50epochs\\vits\\weights\\tiny\\vit_fusion_tiny_4_heads_TEST.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[0mcnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11352\\357034726.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, patch_size, hidden_dim, num_heads, num_layers)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRearrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'b c h w -> b (h w) c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpositional_encoding1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_patches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Rearrange' is not defined"
     ]
    }
   ],
   "source": [
    "def predictions(vit_model, cnn_model, nrows, X_test_VALID, y_test_VALID, X_test_MIDAIR, y_test_midair):\n",
    "    nrows = nrows\n",
    "    ncolumns = 4\n",
    "    subplot_width = 2.7\n",
    "    subplot_height = 2.7 \n",
    "    extra_width_space = 0.5\n",
    "    extra_height_space = 0.5\n",
    "    \n",
    "    fig_width = ncolumns*subplot_width + extra_width_space\n",
    "    fig_height = nrows*subplot_height + extra_height_space\n",
    "    fig_size = (fig_width, fig_height)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    testing_dataset_VALID = MyDataset(X_test_VALID,y_test_VALID, None,0, 42)\n",
    "    testing_dl_VALID = DataLoader(testing_dataset_VALID,batch_size=1)\n",
    "    \n",
    "    testing_dataset_MIDAIR = MyDataset(X_test_MIDAIR,y_test_MIDAIR,None,0,42)\n",
    "    testing_dl_MIDAIR = DataLoader(testing_dataset_MIDAIR,batch_size=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    fig1, axes1 = plt.subplots(nrows,ncolumns, figsize=fig_size)\n",
    "    fig2, axes2 = plt.subplots(nrows,ncolumns, figsize=fig_size)\n",
    "    \n",
    "    \n",
    "    fig1.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "    #loss = nn.\n",
    "    loss_function = WeightedRMSELoss1x().to(device)\n",
    "    #MIDAIR LOOP\n",
    "    vit_model.eval()\n",
    "    cnn_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for idx, (image, depth) in enumerate(testing_dl_MIDAIR):   \n",
    "            image, depth = image.to(device), depth.to(device)\n",
    "            if idx == nrows:\n",
    "                break\n",
    "\n",
    "            vit_depth_pred = vit_model(image).squeeze(0) \n",
    "            cnn_depth_pred = cnn_model(image).squeeze(0)\n",
    "\n",
    "            \n",
    "            image[:,0,:,:] = image[:,0,:,:]*72.5+79.88429260253906\n",
    "            image[:,1,:,:] = image[:,1,:,:]*67.4375+112.80674743652344\n",
    "            image[:,2,:,:] = image[:,2,:,:]*72.4375+109.85155487060547\n",
    "            \n",
    "\n",
    "            image_normalized = (image.cpu().squeeze(0))/287\n",
    "            \n",
    "            axes1[idx,0].imshow(image_normalized.squeeze(0).permute(1,2,0))\n",
    "            axes1[idx,0].set_title(f\"Original_Image{idx}\")\n",
    "            axes1[idx,0].axis(\"off\")\n",
    "            \n",
    "            \n",
    "            img2 = axes1[idx,1].imshow(reverse_norm(vit_depth_pred.cpu().numpy()), vmin=0, vmax=reverse_norm(1))\n",
    "            axes1[idx,1].set_title(\"ViT_Prediction\")\n",
    "            axes1[idx,1].axis(\"off\")\n",
    "            fig1.colorbar(img2,ax=axes1[idx,1])\n",
    "            \n",
    "        \n",
    "            img3 = axes1[idx,2].imshow(reverse_norm(cnn_depth_pred.cpu().numpy()), vmin=0, vmax=reverse_norm(1))\n",
    "            axes1[idx,2].set_title(\"CNN_Prediction\")\n",
    "            axes1[idx,2].axis(\"off\")\n",
    "            fig1.colorbar(img3,ax=axes1[idx,2])\n",
    "            \n",
    "            img4 = axes1[idx,3].imshow(reverse_norm(depth.squeeze(0).squeeze(0).cpu().numpy()), vmin=0, vmax=reverse_norm(1))\n",
    "            axes1[idx,3].set_title(\"Ground_Truth\")\n",
    "            axes1[idx,3].axis(\"off\")\n",
    "            fig1.colorbar(img4,ax=axes1[idx,3])\n",
    "       \n",
    "    fig1.show()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for idx, (image, depth) in enumerate(testing_dl_VALID):   \n",
    "            image, depth = image.to(device), depth.to(device)\n",
    "            if idx == nrows:\n",
    "                break\n",
    "\n",
    "            vit_depth_pred = vit_model(image).squeeze(0)   \n",
    "            cnn_depth_pred = cnn_model(image).squeeze(0)\n",
    "\n",
    "            image[:,0,:,:] = image[:,0,:,:]*72.5+79.88429260253906\n",
    "            image[:,1,:,:] = image[:,1,:,:]*67.4375+112.80674743652344\n",
    "            image[:,2,:,:] = image[:,2,:,:]*72.4375+109.85155487060547\n",
    "            \n",
    "            image_normalized = (image.cpu().squeeze(0))/287\n",
    "            \n",
    "            axes2[idx,0].imshow(image_normalized.squeeze(0).permute(1,2,0))\n",
    "            axes2[idx,0].set_title(f\"Original_Image{idx}\")\n",
    "            axes2[idx,0].axis(\"off\")\n",
    "            \n",
    "            \n",
    "            img2 = axes2[idx,1].imshow(reverse_norm(vit_depth_pred.cpu().numpy()), vmin=0, vmax=reverse_norm(1))\n",
    "            axes2[idx,1].set_title(\"ViT_Prediction\")\n",
    "            axes2[idx,1].axis(\"off\")\n",
    "            fig1.colorbar(img2,ax=axes2[idx,1])\n",
    "            \n",
    "        \n",
    "            img3 = axes2[idx,2].imshow(reverse_norm(cnn_depth_pred.cpu().numpy()), vmin=0, vmax=reverse_norm(1))\n",
    "            axes2[idx,2].set_title(\"CNN_Prediction\")\n",
    "            axes2[idx,2].axis(\"off\")\n",
    "            fig1.colorbar(img3,ax=axes2[idx,2])\n",
    "            \n",
    "            img4 = axes2[idx,3].imshow(reverse_norm(depth.squeeze(0).squeeze(0).cpu().numpy()), vmin=0, vmax=reverse_norm(1))\n",
    "            axes2[idx,3].set_title(\"Ground_Truth\")\n",
    "            axes2[idx,3].axis(\"off\")\n",
    "            fig1.colorbar(img4,ax=axes2[idx,3])\n",
    "       \n",
    "    fig2.show()\n",
    "device = torch.device(\"cuda\")  \n",
    "vit = ViT_fusion_4_heads().to(device)\n",
    "vit.load_state_dict(torch.load(r\"50epochs\\vits\\weights\\tiny\\vit_fusion_tiny_4_heads_TEST.pt\"))\n",
    "cnn = CNN().to(device)\n",
    "cnn.load_state_dict(torch.load(r\"50epochs\\cnns\\weights\\tiny\\cnn_tiny_TEST.pt\"))\n",
    "predictions(vit,cnn,1,X_test_VALID, y_test_VALID, X_test_MIDAIR, y_test_MIDAIR)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f110d070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f7653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ddffa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b5d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca230413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be30b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5963e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa413aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0fdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709950a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7289ac44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2408781070980049\n",
      "0.40179636686158887\n",
      "0.3806677993546303\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_over_1_normalized(load, optim_split, loss_f, epochs, x_train, y_train, X_test_VALID, y_test_VALID, X_test_MIDAIR, y_test_MIDAIR, data_size, batch_size, model, lr, model_type, file_name, l2=True, dropout_bool=False, hidden_layers= 0, nheads=0, encoder_layers=0,transform=None):\n",
    "\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(load, map_location=device))\n",
    "    #for name, param in model.named_parameters():\n",
    "        #print(f\"Parameter: {name}\")\n",
    "        #print(f\"Shape: {param.shape}\")\n",
    "        #print(f\"Initial values:\\n{param.data}\")\n",
    "        #print(\"------------------\")\n",
    "\n",
    "    loss_function = loss_f.to(device)\n",
    "    initial_lr = lr\n",
    "    step_size = 10  # Number of epochs before reducing the learning rate\n",
    "    gamma = 0.66\n",
    "    \n",
    "    if l2:\n",
    "        if optim_split:\n",
    "            optimizer = get_optimizer(model,1e-5)\n",
    "            print(\"get_optimizer_chosen_l2\")\n",
    "        else:\n",
    "            optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=lr/10)\n",
    "    else:\n",
    "        if optim_split:\n",
    "            optimizer = get_optimizer(model,0)\n",
    "            print(\"get_optimizer_chosen\")\n",
    "        else:\n",
    "            optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=0)\n",
    "            \n",
    "    # Initialize the StepLR scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "    avg_train_loss_per_epoch_list = []\n",
    "    \n",
    "    testing_dataset_VALID = MyDataset(X_test_VALID,y_test_VALID, None,0,42)\n",
    "    testing_dl_VALID = DataLoader(testing_dataset_VALID,batch_size=16)\n",
    "    \n",
    "    testing_dataset_MIDAIR = MyDataset(X_test_MIDAIR,y_test_MIDAIR,None,0,42)\n",
    "    testing_dl_MIDAIR = DataLoader(testing_dataset_MIDAIR,batch_size=16)\n",
    "    \n",
    "    VALID_rmse_list = []\n",
    "    VALID_delta1_list, VALID_delta2_list, VALID_delta3_list = [],[],[]\n",
    "    VALID_mae_list, VALID_absrel_list = [],[]\n",
    "    MIDAIR_rmse_list = []\n",
    "    MIDAIR_delta1_list, MIDAIR_delta2_list, MIDAIR_delta3_list = [],[],[]\n",
    "    MIDAIR_mae_list, MIDAIR_absrel_list = [],[]\n",
    "\n",
    "    \n",
    "    #start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    VALID_rmse = 0\n",
    "    VALID_delta1, VALID_delta2, VALID_delta3 = 0, 0, 0\n",
    "    VALID_mae, VALID_absrel = 0, 0\n",
    "\n",
    "    valid_over_1 = 0\n",
    "    total_valid = 0\n",
    "    \n",
    "    midair_over_1 = 0\n",
    "    total_midair = 0\n",
    "    #Setting model to evaluation mode\n",
    "    model.eval()\n",
    "    # Validation / testing loop\n",
    "    with torch.no_grad():\n",
    "        for j, (images, depth_images) in enumerate(testing_dl_VALID):\n",
    "\n",
    "            # Moving images and depth images to GPU\n",
    "            images,depth_images = images.to(device), depth_images.to(device)\n",
    "\n",
    "            #forward pass\n",
    "            outputs = model(images).unsqueeze(1)\n",
    "\n",
    "            mask = torch.where(depth_images==1,1,0)\n",
    "            mask_sum = torch.sum(mask).item()\n",
    "           \n",
    "            valid_over_1 += mask_sum\n",
    "            total_valid += mask.numel()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Calculating loss\n",
    "            loss = WeightedRMSELoss1x()(outputs,depth_images)\n",
    "            #print(loss)\n",
    "            batch_loss = loss.item()\n",
    "\n",
    "            VALID_rmse += batch_loss\n",
    "    \n",
    "    valid_r = valid_over_1/total_valid\n",
    "    print(valid_over_1/total_valid)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "    MIDAIR_rmse = 0\n",
    "    MIDAIR_delta1, MIDAIR_delta2, MIDAIR_delta3 = 0, 0, 0\n",
    "    MIDAIR_mae, MIDAIR_absrel = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, (images, depth_images) in enumerate(testing_dl_MIDAIR):\n",
    "\n",
    "            # Moving images and depth images to GPU\n",
    "            images,depth_images = images.to(device), depth_images.to(device)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            #forward pass\n",
    "            outputs = model(images).unsqueeze(1)\n",
    "\n",
    "            mask = torch.where(depth_images==1,1,0)\n",
    "            mask_sum = torch.sum(mask).item()\n",
    "           \n",
    "            midair_over_1 += mask_sum\n",
    "            total_midair += mask.numel()\n",
    "\n",
    "            \n",
    "            # Calculating loss\n",
    "            loss = WeightedRMSELoss1x()(outputs,depth_images)\n",
    "            batch_loss = loss.item()\n",
    "\n",
    "            MIDAIR_rmse += batch_loss\n",
    "\n",
    "    midair_r = midair_over_1/total_midair\n",
    "    print(midair_over_1/total_midair)\n",
    "    print((valid_r*(0.1313)+(midair_r)*(1-0.1313)))\n",
    "    print(\"\\n\")\n",
    "    return valid_r,midair_r,(valid_r*(0.1313)+(midair_r)*(1-0.1313))\n",
    "run_e = test2(r\"50epochs\\vits\\weights\\extreme\\vit_fusion_extreme_4_heads_TEST.pt\",False,WeightedRMSELoss1x(),50, X_train, y_train, X_test_VALID, y_test_VALID, X_test_MIDAIR,y_test_MIDAIR, \"extreme\", 16, ViT_fusion_4_heads(), 0.00005, \"vit\", \"vit_mse\", False, False, 512,4,16,transform_no_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c0f949ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[241 250 876   4 118 800 373  64 145 223 238 176 778 852 281  62 216 853\n",
      " 826 794 688 460 928 609 104  98 510 384 404 822]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"r1 = np.random.randint(0,len(X_test_VALID),30)\n",
    "r2 = np.random.randint(0,len(X_test_MIDAIR),30)\n",
    "print(r1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "861bbb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r1 = np.random.randint(0,len(X_test_VALID),30)\\n# Use a for loop to create your 16 subplots\\n# Create a new figure\\nfig = plt.figure(figsize=(14, 16))\\nfor i in range(30):\\n    ax = fig.add_subplot(6, 5, i+1)\\n    ax.imshow(np.transpose(X_test_VALID[r1[i]], (1,2,0)))\\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\\n    ax.axis(\\'off\\')  # Turn off the axis labels for each subplot\\n    \\nfig.tight_layout()\\nfig.savefig(\"30-images-valid\", bbox_inches=\\'tight\\')'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"r1 = np.random.randint(0,len(X_test_VALID),30)\n",
    "# Use a for loop to create your 16 subplots\n",
    "# Create a new figure\n",
    "fig = plt.figure(figsize=(14, 16))\n",
    "for i in range(30):\n",
    "    ax = fig.add_subplot(6, 5, i+1)\n",
    "    ax.imshow(np.transpose(X_test_VALID[r1[i]], (1,2,0)))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    ax.axis('off')  # Turn off the axis labels for each subplot\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig.savefig(\"30-images-valid\", bbox_inches='tight')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b4582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cc255d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fig1 = plt.figure(figsize=(14, 16))\\nfor i in range(30):\\n    ax = fig1.add_subplot(6, 5, i+1)\\n    ax.imshow(np.transpose(X_test_MIDAIR[r1[i]], (1,2,0)))\\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\\n    ax.axis(\\'off\\')  # Turn off the axis labels for each subplot\\n    \\nfig1.tight_layout()\\nfig1.savefig(\"30-images-midair\", bbox_inches=\\'tight\\')\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"fig1 = plt.figure(figsize=(14, 16))\n",
    "for i in range(30):\n",
    "    ax = fig1.add_subplot(6, 5, i+1)\n",
    "    ax.imshow(np.transpose(X_test_MIDAIR[r1[i]], (1,2,0)))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    ax.axis('off')  # Turn off the axis labels for each subplot\n",
    "    \n",
    "fig1.tight_layout()\n",
    "fig1.savefig(\"30-images-midair\", bbox_inches='tight')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a5e675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
