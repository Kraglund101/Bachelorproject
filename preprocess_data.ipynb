{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c10b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.utils import make_grid\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103a7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0fee0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0.dev20230317+cu118'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "793080c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #x = torch.Tensor(self.features[idx])\n",
    "        #y = torch.Tensor(self.targets[idx])\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12389647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_statistics(VALID_images,VALID_depth_images,MIDAIR_images,MIDAIR_depth_images):\n",
    "\n",
    "    \n",
    "    #loading both dataset into seperate dataset classes\n",
    "    dataset1 = MyDataset(VALID_images, VALID_depth_images)\n",
    "    dataset2 = MyDataset(MIDAIR_images, MIDAIR_depth_images)\n",
    "    \n",
    "    # concatenating training, validation and testing images into one tensor\n",
    "    X_combined = torch.cat((dataset1.X,dataset2.X),dim=0)\n",
    "    y_combined = torch.cat((dataset1.y,dataset2.y),dim=0)\n",
    "    \n",
    "    for channel in range(X_combined.shape[1]):\n",
    "        values_in_channel = X_combined[:,channel,:,:]\n",
    "        min_value = torch.min(values_in_channel)\n",
    "        max_value = torch.max(values_in_channel)\n",
    "        mean_value = torch.mean(values_in_channel.float())\n",
    "        std_value = torch.std(values_in_channel)\n",
    "        print(f\"X-data channel {channel+1}\")\n",
    "        print(f\"  Min: {min_value.item()}\")\n",
    "        print(f\"  Max: {max_value.item()}\")\n",
    "        print(f\"  Mean: {mean_value.item()}\")\n",
    "        print(f\"  Standard Deviation: {std_value.item()}\")\n",
    "        X_combined[:,channel,:,:] = (values_in_channel-mean_value)/std_value\n",
    "    print(torch.max(X_combined))   \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    for channel in range(y_combined.shape[1]):\n",
    "        values_in_channel = y_combined[:,channel,:,:]\n",
    "        min_value = torch.min(values_in_channel)\n",
    "        max_value = torch.max(values_in_channel)\n",
    "        mean_value = torch.mean(values_in_channel.float())\n",
    "        std_value = torch.std(values_in_channel)\n",
    "        print(f\"y-data channel {channel+1}\")\n",
    "        print(f\"  Min: {min_value.item()}\")\n",
    "        print(f\"  Max: {max_value.item()}\")\n",
    "        print(f\"  Mean: {mean_value.item()}\")\n",
    "        print(f\"  Standard Deviation: {std_value.item()}\")\n",
    "        y_combined[:,channel,:,:] = (values_in_channel-mean_value)/std_value\n",
    "        \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ecc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(VALID_images,VALID_depth_images,MIDAIR_images,MIDAIR_depth_images):\n",
    "    #loading both dataset into seperate dataset classes\n",
    "    dataset1 = MyDataset(VALID_images, VALID_depth_images)\n",
    "    dataset2 = MyDataset(MIDAIR_images, MIDAIR_depth_images)\n",
    "    \n",
    "     #splitting the dataset \n",
    "    X_train_VALID, X_temp_VALID, y_train_VALID, y_temp_VALID = train_test_split(dataset1.X, dataset1.y, test_size=0.3, random_state=44)\n",
    "    X_val_VALID, X_test_VALID, y_val_VALID, y_test_VALID = train_test_split(X_temp_VALID,y_temp_VALID, test_size=0.5, random_state=44)\n",
    "    \n",
    "    X_train_MIDAIR, X_temp_MIDAIR, y_train_MIDAIR, y_temp_MIDAIR = train_test_split(dataset2.X, dataset2.y, test_size=0.3, random_state=44)\n",
    "    X_val_MIDAIR, X_test_MIDAIR, y_val_MIDAIR, y_test_MIDAIR = train_test_split(X_temp_MIDAIR,y_temp_MIDAIR, test_size=0.5, random_state=44)\n",
    "    \n",
    "    X_combined_train = torch.cat((X_train_VALID, X_train_MIDAIR),dim=0)\n",
    "    X_combined_val = torch.cat((X_val_VALID, X_val_MIDAIR),dim=0)\n",
    "    \n",
    "   \n",
    "    y_combined_train = torch.cat((y_train_VALID, y_train_MIDAIR),dim=0)\n",
    "    y_combined_val = torch.cat((y_val_VALID, y_val_MIDAIR),dim=0)\n",
    "                               \n",
    "    \n",
    "    for channel in range(X_combined_train.shape[1]):\n",
    "        values_in_channel = X_combined_train[:,channel,:,:]\n",
    "        min_value = torch.min(values_in_channel)\n",
    "        max_value = torch.max(values_in_channel)\n",
    "        mean_value = torch.mean(values_in_channel.float())\n",
    "        std_value = torch.std(values_in_channel)\n",
    "        print(f\"X-data channel {channel+1}\")\n",
    "        print(f\"  Min: {min_value.item()}\")\n",
    "        print(f\"  Max: {max_value.item()}\")\n",
    "        print(f\"  Mean: {mean_value.item()}\")\n",
    "        print(f\"  Standard Deviation: {std_value.item()}\")\n",
    "        X_combined_train[:,channel,:,:] = (X_combined_train[:,channel,:,:]-mean_value)/std_value                     \n",
    "        X_combined_val[:,channel,:,:] = (X_combined_val[:,channel,:,:]-mean_value)/std_value\n",
    "        X_test_VALID[:,channel,:,:] = (X_test_VALID[:,channel,:,:]-mean_value)/std_value\n",
    "        X_test_MIDAIR[:,channel,:,:] = (X_test_MIDAIR[:,channel,:,:]-mean_value)/std_value\n",
    "                               \n",
    "                            \n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    for channel in range(y_combined_train.shape[1]):\n",
    "        values_in_channel = y_combined_train[:,channel,:,:]\n",
    "        min_value = torch.min(values_in_channel)\n",
    "        max_value = torch.max(values_in_channel)\n",
    "        mean_value = torch.mean(values_in_channel.float())\n",
    "        std_value = torch.std(values_in_channel)\n",
    "        print(f\"y-data channel {channel+1}\")\n",
    "        print(f\"  Min: {min_value.item()}\")\n",
    "        print(f\"  Max: {max_value.item()}\")\n",
    "        print(f\"  Mean: {mean_value.item()}\")\n",
    "        print(f\"  Standard Deviation: {std_value.item()}\")\n",
    "        y_combined_train[:,channel,:,:] = (y_combined_train[:,channel,:,:]-mean_value)/std_value                     \n",
    "        y_combined_val[:,channel,:,:] = (y_combined_val[:,channel,:,:]-mean_value)/std_value\n",
    "        y_test_VALID[:,channel,:,:] = (y_test_VALID[:,channel,:,:]-mean_value)/std_value\n",
    "        y_test_MIDAIR[:,channel,:,:] = (y_test_MIDAIR[:,channel,:,:]-mean_value)/std_value\n",
    "    \n",
    "\n",
    "    \n",
    "    return (X_combined_train.half(), y_combined_train.half()), (X_combined_val.half(), y_combined_val.half()), (X_test_VALID.half(), y_test_VALID.half()), (X_test_MIDAIR.half(), y_test_MIDAIR.half())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b43b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization_normalization(VALID_images,VALID_depth_images,MIDAIR_images,MIDAIR_depth_images):\n",
    "    #loading both dataset into seperate dataset classes\n",
    "    dataset1 = MyDataset(VALID_images, VALID_depth_images)\n",
    "    dataset2 = MyDataset(MIDAIR_images, MIDAIR_depth_images)\n",
    "    \n",
    "     #splitting the dataset \n",
    "    X_train_VALID, X_temp_VALID, y_train_VALID, y_temp_VALID = train_test_split(dataset1.X, dataset1.y, test_size=0.3, random_state=44)\n",
    "    X_val_VALID, X_test_VALID, y_val_VALID, y_test_VALID = train_test_split(X_temp_VALID,y_temp_VALID, test_size=0.5, random_state=44)\n",
    "    \n",
    "    X_train_MIDAIR, X_temp_MIDAIR, y_train_MIDAIR, y_temp_MIDAIR = train_test_split(dataset2.X, dataset2.y, test_size=0.3, random_state=44)\n",
    "    X_val_MIDAIR, X_test_MIDAIR, y_val_MIDAIR, y_test_MIDAIR = train_test_split(X_temp_MIDAIR,y_temp_MIDAIR, test_size=0.5, random_state=44)\n",
    "    \n",
    "    X_combined_train = torch.cat((X_train_VALID, X_train_MIDAIR),dim=0)\n",
    "    X_combined_val = torch.cat((X_val_VALID, X_val_MIDAIR),dim=0)\n",
    "    #HERE\n",
    "    print(\"valid y\",y_train_VALID.shape)\n",
    "    print(\"midair y\",y_train_MIDAIR.shape)\n",
    "    y_combined_train = torch.cat((y_train_VALID, y_train_MIDAIR),dim=0)\n",
    "    y_combined_val = torch.cat((y_val_VALID, y_val_MIDAIR),dim=0)\n",
    "                               \n",
    "    \n",
    "    for channel in range(X_combined_train.shape[1]):\n",
    "        values_in_channel = X_combined_train[:,channel,:,:]\n",
    "        min_value = torch.min(values_in_channel)\n",
    "        max_value = torch.max(values_in_channel)\n",
    "        mean_value = torch.mean(values_in_channel.float())\n",
    "        std_value = torch.std(values_in_channel)\n",
    "        #print(f\"X-data channel {channel+1}\")\n",
    "        #print(f\"  Min: {min_value.item()}\")\n",
    "        #print(f\"  Max: {max_value.item()}\")\n",
    "        #print(f\"  Mean: {mean_value.item()}\")\n",
    "        #print(f\"  Standard Deviation: {std_value.item()}\")\n",
    "        X_combined_train[:,channel,:,:] = (X_combined_train[:,channel,:,:]-mean_value)/std_value                     \n",
    "        X_combined_val[:,channel,:,:] = (X_combined_val[:,channel,:,:]-mean_value)/std_value\n",
    "        X_test_VALID[:,channel,:,:] = (X_test_VALID[:,channel,:,:]-mean_value)/std_value\n",
    "        X_test_MIDAIR[:,channel,:,:] = (X_test_MIDAIR[:,channel,:,:]-mean_value)/std_value\n",
    "                               \n",
    "                            \n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    min_value = torch.min(y_combined_train)\n",
    "    max_value = torch.max(y_combined_train)\n",
    "    y_combined_train = (y_combined_train-min_value)/(max_value-min_value)                   \n",
    "    y_combined_val = (y_combined_val-min_value)/(max_value-min_value)\n",
    "    y_test_VALID = (y_test_VALID-min_value)/(max_value-min_value)\n",
    "    y_test_MIDAIR = (y_test_MIDAIR-min_value)/(max_value-min_value)\n",
    "\n",
    "\n",
    "    \n",
    "    return (X_combined_train.half(), y_combined_train.half()), (X_combined_val.half(), y_combined_val.half()), (X_test_VALID.half(), y_test_VALID.half()), (X_test_MIDAIR.half(), y_test_MIDAIR.half())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "954cbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_feature_scaling(training_tensor, validation_tensor, testing_tensor_VALID, testing_tensor_MIDAIR):\n",
    "    tensor_min = torch.min(training_tensor) \n",
    "    tensor_max = torch.max(training_tensor)\n",
    "    \n",
    "    training_tensor_scaled = (training_tensor-tensor_min)/(tensor_max-tensor_min)\n",
    "    validation_tensor_scaled = (validation_tensor-tensor_min)/(tensor_max-tensor_min)\n",
    "    \n",
    "    testing_tensor_VALID_scaled = (testing_tensor_VALID-tensor_min)/(tensor_max-tensor_min)\n",
    "    testing_tensor_MIDAIR_scaled = (testing_tensor_MIDAIR-tensor_min)/(tensor_max-tensor_min)\n",
    "    #testing_tensor_scaled = (testing_tensor-tensor_min)/(tensor_max-tensor_min)\n",
    "    \n",
    "    return training_tensor_scaled.half(),validation_tensor_scaled.half(), testing_tensor_VALID_scaled.half(), testing_tensor_MIDAIR_scaled.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed061213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating  tiny\n",
      "torch.Size([180, 1, 256, 256])\n",
      "valid y torch.Size([18, 1, 256, 256])\n",
      "midair y torch.Size([126, 1, 256, 256])\n",
      "\n",
      "\n",
      "Finished tiny\n",
      "Initiating  xxsmall\n",
      "torch.Size([360, 1, 256, 256])\n",
      "valid y torch.Size([35, 1, 256, 256])\n",
      "midair y torch.Size([252, 1, 256, 256])\n",
      "\n",
      "\n",
      "Finished xxsmall\n"
     ]
    }
   ],
   "source": [
    "def normalization_and_standardization(VALID_images,VALID_depth_images,size, max_val=None):\n",
    "#size = [small,xsmall,medium,large,xlarge]\n",
    "    \n",
    "    np.random.seed(1337)\n",
    "    size = size.lower()\n",
    "    \n",
    "    if size == \"tiny\":\n",
    "        l1,l2 = round(196*0.13134),196\n",
    "    elif size == \"xxsmall\":\n",
    "        l1,l2 = round(391*0.13134),391\n",
    "    elif size == \"xsmall\":\n",
    "        l1,l2 = round(782*0.13134),782\n",
    "    elif size == \"small\":\n",
    "        l1,l2 = round(1563*0.13134),1563\n",
    "    elif size == \"medium\":\n",
    "        l1,l2 = round(3125*0.13134),3125\n",
    "    elif size == \"large\":\n",
    "        l1,l2 = round(6250*0.13134),6250\n",
    "    elif size == \"xlarge\":\n",
    "        l1,l2 = round(12500*0.13134),12500\n",
    "    elif size == \"xxlarge\":\n",
    "        l1,l2 = round(25000*0.13134),25000\n",
    "    elif size == \"extreme\":\n",
    "        l1,l2 = 6567,50000\n",
    "       \n",
    "    #Check if folder exists\n",
    "    if not os.path.exists(f\"Fused_normalized_data/{size}\") or not os.path.exists(f\"Normalized_standardized_data_{max_val}_new/{size}\"):\n",
    "        #if not create folder\n",
    "        \n",
    "        #load images\n",
    "        indicies = np.random.choice([i for i in range(6567)],size=l1,replace=False)\n",
    "        VALID_images = torch.load(VALID_images)[indicies]\n",
    "        VALID_depth_images = torch.load(VALID_depth_images)[indicies]\n",
    "        MIDAIR_images = torch.load(f\"C:/Users/Frede/Desktop/bachelorprojekt/MIDAIR_tensors/{l2}_images/MIDAIR_tensor_images.pt\")\n",
    "        MIDAIR_depth_images = torch.load(f\"C:/Users/Frede/Desktop/bachelorprojekt/MIDAIR_tensors/{l2}_images/MIDAIR_tensor_depth_images_no_rgb.pt\")\n",
    "        print(MIDAIR_depth_images.shape)\n",
    "        if max_val != None:\n",
    "            if not os.path.exists(f\"Normalized_standardized_data_{max_val}_new/{size}\"):\n",
    "                os.makedirs(f\"Normalized_standardized_data_{max_val}_new/{size}\")\n",
    "                #clip datasets\n",
    "\n",
    "                VALID_depth_images_np = np.clip(VALID_depth_images.numpy(), 0.001, max_val, VALID_depth_images.numpy())\n",
    "                MIDAIR_depth_images_np = np.clip(MIDAIR_depth_images.numpy(), 0.001, max_val, MIDAIR_depth_images.numpy())\n",
    "\n",
    "                VALID_depth_images = torch.from_numpy(VALID_depth_images_np)\n",
    "                MIDAIR_depth_images = torch.from_numpy(MIDAIR_depth_images_np)\n",
    "\n",
    "                #normalize datasets and combine them\n",
    "                dataset = standardization_normalization(VALID_images, VALID_depth_images, MIDAIR_images, MIDAIR_depth_images)\n",
    "                #index them and save them to folder\n",
    "                X_train,y_train,X_val,y_val,X_test_VALID,y_test_VALID, X_test_MIDAIR, y_test_MIDAIR = dataset[0][0], dataset[0][1], dataset[1][0], dataset[1][1], dataset[2][0], dataset[2][1], dataset[3][0], dataset[3][1]\n",
    "                torch.save(X_train, f\"Normalized_standardized_data_{max_val}_new/{size}/X_train.pt\")\n",
    "                torch.save(y_train, f\"Normalized_standardized_data_{max_val}_new/{size}/y_train.pt\")\n",
    "                torch.save(X_val, f\"Normalized_standardized_data_{max_val}_new/{size}/X_val.pt\")\n",
    "                torch.save(y_val, f\"Normalized_standardized_data_{max_val}_new/{size}/y_val.pt\")\n",
    "                torch.save(X_test_VALID, f\"Normalized_standardized_data_{max_val}_new/{size}/X_test_VALID.pt\")\n",
    "                torch.save(y_test_VALID, f\"Normalized_standardized_data_{max_val}_new/{size}/y_test_VALID.pt\")\n",
    "                torch.save(X_test_MIDAIR, f\"Normalized_standardized_data_{max_val}_new/{size}/X_test_MIDAIR.pt\")\n",
    "                torch.save(y_test_MIDAIR, f\"Normalized_standardized_data_{max_val}_new/{size}/y_test_MIDAIR.pt\")\n",
    "\n",
    "            else:\n",
    "                VALID_depth_images_np = np.clip(VALID_depth_images.numpy(), 0.001, max_val, VALID_depth_images.numpy())\n",
    "                MIDAIR_depth_images_np = np.clip(MIDAIR_depth_images.numpy(), 0.001, max_val, MIDAIR_depth_images.numpy())\n",
    "\n",
    "                VALID_depth_images = torch.from_numpy(VALID_depth_images_np)\n",
    "                MIDAIR_depth_images = torch.from_numpy(MIDAIR_depth_images_np)\n",
    "               \n",
    "                #normalize datasets and combine them\n",
    "                dataset = standardization_normalization(VALID_images, VALID_depth_images, MIDAIR_images, MIDAIR_depth_images)\n",
    "                #index them and save them to folder\n",
    "                X_train,y_train,X_val,y_val,X_test_VALID,y_test_VALID, X_test_MIDAIR, y_test_MIDAIR = dataset[0][0], dataset[0][1], dataset[1][0], dataset[1][1], dataset[2][0], dataset[2][1], dataset[3][0], dataset[3][1]\n",
    "                torch.save(X_train, f\"Normalized_standardized_data_{max_val}_new/{size}/X_train.pt\")\n",
    "                torch.save(y_train, f\"Normalized_standardized_data_{max_val}_new/{size}/y_train.pt\")\n",
    "                torch.save(X_val, f\"Normalized_standardized_data_{max_val}_new/{size}/X_val.pt\")\n",
    "                torch.save(y_val, f\"Normalized_standardized_data_{max_val}_new/{size}/y_val.pt\")\n",
    "                torch.save(X_test_VALID, f\"Normalized_standardized_data_{max_val}_new/{size}/X_test_VALID.pt\")\n",
    "                torch.save(y_test_VALID, f\"Normalized_standardized_data_{max_val}_new/{size}/y_test_VALID.pt\")\n",
    "                torch.save(X_test_MIDAIR, f\"Normalized_standardized_data_{max_val}_new/{size}/X_test_MIDAIR.pt\")\n",
    "                torch.save(y_test_MIDAIR, f\"Normalized_standardized_data_{max_val}_new/{size}/y_test_MIDAIR.pt\")\n",
    "            return X_train, X_val, y_train, y_val, X_test_VALID, y_test_VALID, X_test_MIDAIR, y_test_MIDAIR \n",
    "        \n",
    "            \n",
    "VALID_images_path = r\"C:/Users/Frede/Desktop/bachelorprojekt/VALID_tensors/VALID_tensor_images.pt\"\n",
    "VALID_depth_images_path_no_rgb = r\"C:/Users/Frede/Desktop/bachelorprojekt/VALID_tensors/VALID_tensor_depth_images_no_rgb.pt\"\n",
    "\n",
    "\n",
    "\n",
    "sizes = [\"tiny\",\"xxsmall\"]\n",
    "#sizes = [\"extreme\"]\n",
    "for s in sizes:\n",
    "    print(\"Initiating \", s)\n",
    "    normalization_and_standardization(VALID_images_path,VALID_depth_images_path_no_rgb,s,80)\n",
    "    print(\"Finished\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03f51ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid y torch.Size([350, 1, 256, 256])\n",
      "midair y torch.Size([378, 1, 256, 256])\n",
      "X-data channel 1\n",
      "  Min: 0.0\n",
      "  Max: 255.0\n",
      "  Mean: 57.44839096069336\n",
      "  Standard Deviation: 66.3125\n",
      "X-data channel 2\n",
      "  Min: 0.0\n",
      "  Max: 255.0\n",
      "  Mean: 87.5497817993164\n",
      "  Standard Deviation: 67.1875\n",
      "X-data channel 3\n",
      "  Min: 0.0\n",
      "  Max: 255.0\n",
      "  Mean: 90.70647430419922\n",
      "  Standard Deviation: 69.8125\n",
      "\n",
      "\n",
      "y-data channel 1\n",
      "  Min: 1.5166015625\n",
      "  Max: 80.0\n",
      "  Mean: 62.861209869384766\n",
      "  Standard Deviation: 27.65625\n"
     ]
    }
   ],
   "source": [
    "def Standardization_only(VALID_images,VALID_depth_images,size, max_val=None):\n",
    "#size = [small,xsmall,medium,large,xlarge]\n",
    "    \n",
    "\n",
    "    size = size.lower()\n",
    "\n",
    "    if size == \"xsmall\":\n",
    "        l1,l2 = 500,500\n",
    "    elif size == \"small\":\n",
    "        l1,l2 = 1000,1000\n",
    "    elif size == \"medium\":\n",
    "        l1,l2 = 2500,2500\n",
    "    elif size == \"large\":\n",
    "        l1,l2 = 6567,7500\n",
    "    elif size == \"xlarge\":\n",
    "        l1,l2 = 6567,12500\n",
    "    elif size == \"xxlarge\":\n",
    "        l1,l2 = 6567,25000\n",
    "    elif size == \"extreme\":\n",
    "        l1,l2 = 6567,50000\n",
    "       \n",
    "    #Check if folder exists\n",
    "    if not os.path.exists(f\"Standardized_data/{size}\") or not os.path.exists(f\"Standardized_data_{max_val}/{size}\"):\n",
    "        #if not create folder\n",
    "        \n",
    "        #load images\n",
    "        VALID_images = torch.load(VALID_images)[0:l1]\n",
    "        VALID_depth_images = torch.load(VALID_depth_images)[0:l1]\n",
    "        MIDAIR_images = torch.load(f\"C:/Users/Frede/Desktop/bachelorprojekt/MIDAIR_tensors/{l2}_images/MIDAIR_tensor_images.pt\")\n",
    "        MIDAIR_depth_images = torch.load(f\"C:/Users/Frede/Desktop/bachelorprojekt/MIDAIR_tensors/{l2}_images/MIDAIR_tensor_depth_images_no_rgb.pt\")\n",
    "        if max_val != None:\n",
    "            if not os.path.exists(f\"Standardized_data_{max_val}/{size}\"):\n",
    "                os.makedirs(f\"Standardized_data_{max_val}/{size}\")\n",
    "                #clip datasets\n",
    "\n",
    "                VALID_depth_images_np = np.clip(VALID_depth_images.numpy(), 0.001, max_val, VALID_depth_images.numpy())\n",
    "                MIDAIR_depth_images_np = np.clip(MIDAIR_depth_images.numpy(), 0.001, max_val, MIDAIR_depth_images.numpy())\n",
    "\n",
    "                VALID_depth_images = torch.from_numpy(VALID_depth_images_np)\n",
    "                MIDAIR_depth_images = torch.from_numpy(MIDAIR_depth_images_np)\n",
    "\n",
    "                #normalize datasets and combine them\n",
    "                dataset = standardization(VALID_images, VALID_depth_images, MIDAIR_images, MIDAIR_depth_images)\n",
    "                #index them and save them to folder\n",
    "                X_train,y_train,X_val,y_val,X_test_VALID,y_test_VALID, X_test_MIDAIR, y_test_MIDAIR = dataset[0][0], dataset[0][1], dataset[1][0], dataset[1][1], dataset[2][0], dataset[2][1], dataset[3][0], dataset[3][1]\n",
    "                torch.save(X_train, f\"Standardized_data_{max_val}/{size}/X_train.pt\")\n",
    "                torch.save(y_train, f\"Standardized_data_{max_val}/{size}/y_train.pt\")\n",
    "                torch.save(X_val, f\"Standardized_data_{max_val}/{size}/X_val.pt\")\n",
    "                torch.save(y_val, f\"Standardized_data_{max_val}/{size}/y_val.pt\")\n",
    "                torch.save(X_test_VALID, f\"Standardized_data_{max_val}/{size}/X_test_VALID.pt\")\n",
    "                torch.save(y_test_VALID, f\"Standardized_data_{max_val}/{size}/y_test_VALID.pt\")\n",
    "                torch.save(X_test_MIDAIR, f\"Standardized_data_{max_val}/{size}/X_test_MIDAIR.pt\")\n",
    "                torch.save(y_test_MIDAIR, f\"Standardized_data_{max_val}/{size}/y_test_MIDAIR.pt\")\n",
    "\n",
    "            else:\n",
    "                VALID_depth_images_np = np.clip(VALID_depth_images.numpy(), 0.001, max_val, VALID_depth_images.numpy())\n",
    "                MIDAIR_depth_images_np = np.clip(MIDAIR_depth_images.numpy(), 0.001, max_val, MIDAIR_depth_images.numpy())\n",
    "\n",
    "                VALID_depth_images = torch.from_numpy(VALID_depth_images_np)\n",
    "                MIDAIR_depth_images = torch.from_numpy(MIDAIR_depth_images_np)\n",
    "\n",
    "                #normalize datasets and combine them\n",
    "                dataset = standardization(VALID_images, VALID_depth_images, MIDAIR_images, MIDAIR_depth_images)\n",
    "                #index them and save them to folder\n",
    "                X_train,y_train,X_val,y_val,X_test_VALID,y_test_VALID, X_test_MIDAIR, y_test_MIDAIR = dataset[0][0], dataset[0][1], dataset[1][0], dataset[1][1], dataset[2][0], dataset[2][1], dataset[3][0], dataset[3][1]\n",
    "                torch.save(X_train, f\"Standardized_data_{max_val}/{size}/X_train.pt\")\n",
    "                torch.save(y_train, f\"Standardized_data_{max_val}/{size}/y_train.pt\")\n",
    "                torch.save(X_val, f\"Standardized_data_{max_val}/{size}/X_val.pt\")\n",
    "                torch.save(y_val, f\"Standardized_data_{max_val}/{size}/y_val.pt\")\n",
    "                torch.save(X_test_VALID, f\"Standardized_data_{max_val}/{size}/X_test_VALID.pt\")\n",
    "                torch.save(y_test_VALID, f\"Standardized_data_{max_val}/{size}/y_test_VALID.pt\")\n",
    "                torch.save(X_test_MIDAIR, f\"Standardized_data_{max_val}/{size}/X_test_MIDAIR.pt\")\n",
    "                torch.save(y_test_MIDAIR, f\"Standardized_data_{max_val}/{size}/y_test_MIDAIR.pt\")\n",
    "            return X_train, X_val, y_train, y_val, X_test_VALID, y_test_VALID, X_test_MIDAIR, y_test_MIDAIR\n",
    "        \n",
    "            \n",
    "VALID_images_path = r\"C:/Users/Frede/Desktop/bachelorprojekt/VALID_tensors/VALID_tensor_images.pt\"\n",
    "VALID_depth_images_path_no_rgb = r\"C:/Users/Frede/Desktop/bachelorprojekt/VALID_tensors/VALID_tensor_depth_images_no_rgb.pt\"\n",
    "\n",
    "\n",
    "\n",
    "#sizes = [\"xsmall\",\"small\",\"medium\",\"large\", \"xlarge\", \"xxlarge\"]\n",
    "#sizes = [\"extreme\"]\n",
    "for s in sizes:\n",
    "    Standardization_only(VALID_images_path,VALID_depth_images_path_no_rgb,s,80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
